{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshojaei77/mshojaei77.github.io/blob/main/labs/01_swiglu_activation_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# SwiGLU Activation Function: From Theory to Implementation\n",
        "\n",
        "**Lab Companion to Chapter 1: Neural Networks**\n",
        "\n",
        "Welcome to the hands-on exploration of SwiGLU, the activation function that's become the secret sauce of modern LLMs! In this notebook, we'll:\n",
        "\n",
        "🎯 **Build SwiGLU from scratch** and understand its mechanics  \n",
        "📊 **Visualize** how it compares to ReLU, GELU, and friends  \n",
        "🚀 **Implement** it in a real Transformer feedforward layer  \n",
        "🔬 **Experiment** with different variants and see the performance impact  \n",
        "\n",
        "By the end of this lab, you'll not only understand why SwiGLU has become the darling of LLM architectures, but you'll have the skills to implement and experiment with it yourself.\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "- Basic understanding of neural networks (covered in Chapter 1)\n",
        "- Familiarity with PyTorch tensors and operations\n",
        "- A sense of curiosity about what makes modern LLMs tick! 🤖\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🛠️ Setup and Imports\n",
        "\n",
        "Let's start by importing the tools we'll need for our activation function adventure:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from typing import Optional\n",
        "import time\n",
        "\n",
        "# Set style for prettier plots\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 📚 Quick Theory Refresher\n",
        "\n",
        "Before we dive into code, let's quickly review what makes SwiGLU special:\n",
        "\n",
        "### The GLU Family Tree\n",
        "\n",
        "**Gated Linear Unit (GLU)**: The grandfather of the family\n",
        "```\n",
        "GLU(x) = σ(xW₁ + b₁) ⊙ (xW₂ + b₂)\n",
        "```\n",
        "Where σ is sigmoid and ⊙ is element-wise multiplication.\n",
        "\n",
        "**SwiGLU**: The modern champion\n",
        "```\n",
        "SwiGLU(x) = Swish(xW₁ + b₁) ⊙ (xW₂ + b₂)\n",
        "where Swish(z) = z × σ(z)\n",
        "```\n",
        "\n",
        "**The Magic**: The gating mechanism allows the network to learn dynamic control over information flow, while the Swish activation provides smooth gradients and can handle small negative values.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🧠 Activation Functions: From Simple to Sophisticated\n",
        "\n",
        "Let's implement and visualize different activation functions to see how SwiGLU fits into the evolution:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    \"\"\"The classic: ReLU activation\"\"\"\n",
        "    return torch.clamp(x, min=0)\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\"GELU: Gaussian Error Linear Unit\"\"\"\n",
        "    return x * 0.5 * (1 + torch.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))\n",
        "\n",
        "def swish_silu(x):\n",
        "    \"\"\"Swish/SiLU: x * sigmoid(x)\"\"\"\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    \"\"\"Leaky ReLU: fixes the dying ReLU problem\"\"\"\n",
        "    return torch.where(x > 0, x, alpha * x)\n",
        "\n",
        "# Let's visualize these functions\n",
        "x = torch.linspace(-5, 5, 1000)\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "activations = {\n",
        "    'ReLU': relu(x),\n",
        "    'Leaky ReLU': leaky_relu(x),\n",
        "    'GELU': gelu(x),\n",
        "    'Swish/SiLU': swish_silu(x)\n",
        "}\n",
        "\n",
        "for i, (name, y) in enumerate(activations.items(), 1):\n",
        "    plt.subplot(2, 2, i)\n",
        "    plt.plot(x.numpy(), y.numpy(), linewidth=2, label=name)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.title(f'{name} Activation', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Input (x)')\n",
        "    plt.ylabel('Output f(x)')\n",
        "    plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Evolution of Activation Functions', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# Let's also plot them together for comparison\n",
        "plt.figure(figsize=(12, 8))\n",
        "for name, y in activations.items():\n",
        "    plt.plot(x.numpy(), y.numpy(), linewidth=2.5, label=name)\n",
        "\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.title('Activation Functions Comparison', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Input (x)', fontsize=12)\n",
        "plt.ylabel('Output f(x)', fontsize=12)\n",
        "plt.legend(fontsize=12)\n",
        "plt.xlim(-4, 4)\n",
        "plt.ylim(-1, 4)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🚀 SwiGLU Implementation\n",
        "\n",
        "Now for the main event! Let's implement SwiGLU from scratch and see it in action:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SwiGLU(nn.Module):\n",
        "    \"\"\"SwiGLU Activation Function\n",
        "    \n",
        "    SwiGLU(x) = Swish(xW1 + b1) ⊙ (xW2 + b2)\n",
        "    where Swish(z) = z * sigmoid(z)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
        "        super().__init__()\n",
        "        # Two linear projections for gating\n",
        "        self.w1 = nn.Linear(in_features, out_features, bias=bias)\n",
        "        self.w2 = nn.Linear(in_features, out_features, bias=bias)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # SwiGLU: SiLU(xW1) ⊙ (xW2)\n",
        "        return F.silu(self.w1(x)) * self.w2(x)\n",
        "\n",
        "\n",
        "# Let's test our implementation\n",
        "input_dim = 4\n",
        "batch_size = 3\n",
        "test_input = torch.randn(batch_size, input_dim)\n",
        "\n",
        "print(\"🧪 Testing SwiGLU Implementation\")\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Input values:\\n{test_input}\")\n",
        "\n",
        "swiglu = SwiGLU(input_dim, input_dim)\n",
        "output = swiglu(test_input)\n",
        "\n",
        "print(f\"\\nOutput shape: {output.shape}\")\n",
        "print(f\"Output values:\\n{output}\")\n",
        "print(f\"\\n✅ SwiGLU working correctly!\")\n",
        "\n",
        "print(\"\\n💡 Exercise 1: Try replacing F.silu with F.gelu or torch.sigmoid!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 📊 Comparing Activation Functions\n",
        "\n",
        "Let's visualize how different gated activations behave and compare their gating patterns:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare point-wise activations first\n",
        "z = torch.linspace(-5, 5, 1000)\n",
        "\n",
        "activations = {\n",
        "    \"ReLU\": lambda x: F.relu(x),\n",
        "    \"GELU\": lambda x: F.gelu(x),\n",
        "    \"SiLU (Swish)\": lambda x: F.silu(x),\n",
        "    \"Sigmoid\": lambda x: torch.sigmoid(x)\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "for name, fn in activations.items():\n",
        "    plt.plot(z, fn(z).detach(), label=name, linewidth=2.5)\n",
        "\n",
        "plt.title(\"Point-wise Activation Functions\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"Input (z)\")\n",
        "plt.ylabel(\"Output\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Now let's create GLU variants for comparison\n",
        "class GLUVariants(nn.Module):\n",
        "    \"\"\"Collection of GLU variants for comparison\"\"\"\n",
        "    \n",
        "    def __init__(self, in_features: int, out_features: int, variant: str = 'swiglu'):\n",
        "        super().__init__()\n",
        "        self.variant = variant.lower()\n",
        "        self.w1 = nn.Linear(in_features, out_features, bias=False)\n",
        "        self.w2 = nn.Linear(in_features, out_features, bias=False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if self.variant == 'glu':\n",
        "            # Original GLU: sigmoid gate\n",
        "            return torch.sigmoid(self.w1(x)) * self.w2(x)\n",
        "        elif self.variant == 'swiglu':\n",
        "            # SwiGLU: SiLU/Swish gate\n",
        "            return F.silu(self.w1(x)) * self.w2(x)\n",
        "        elif self.variant == 'geglu':\n",
        "            # GeGLU: GELU gate\n",
        "            return F.gelu(self.w1(x)) * self.w2(x)\n",
        "        elif self.variant == 'reglu':\n",
        "            # ReGLU: ReLU gate\n",
        "            return F.relu(self.w1(x)) * self.w2(x)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown variant: {self.variant}\")\n",
        "\n",
        "\n",
        "# Test all variants\n",
        "variants = ['glu', 'swiglu', 'geglu', 'reglu']\n",
        "test_input = torch.randn(8, 16)\n",
        "\n",
        "print(\"🎭 Testing GLU Variants\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "for variant in variants:\n",
        "    model = GLUVariants(16, 16, variant)\n",
        "    with torch.no_grad():\n",
        "        output = model(test_input)\n",
        "    print(f\"{variant.upper():<8}: Output shape {output.shape}, Mean: {output.mean():.4f}, Std: {output.std():.4f}\")\n",
        "\n",
        "print(\"\\n✅ All GLU variants working correctly!\")\n",
        "print(\"\\n💡 Challenge 2: Extend the visualization to show gated outputs!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🤖 Integrating SwiGLU into Transformer Feed-Forward Layer\n",
        "\n",
        "Now let's see how SwiGLU fits into real Transformer architectures, just like in LLaMA and other modern LLMs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedForwardSwiGLU(nn.Module):\n",
        "    \"\"\"Modern Transformer FFN with SwiGLU activation\n",
        "    \n",
        "    This is a drop-in replacement for standard Transformer FFN layers.\n",
        "    Used in LLaMA, PaLM, and other state-of-the-art models.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        # Note: We use 2 * d_ff for the intermediate dimension\n",
        "        # This accounts for the gating mechanism\n",
        "        self.fc1 = nn.Linear(d_model, 2 * d_ff, bias=False)  # Gate and Up projections combined\n",
        "        self.fc2 = nn.Linear(d_ff, d_model, bias=False)      # Down projection\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Split the combined projection into gate and up branches\n",
        "        gate, up = self.fc1(x).chunk(2, dim=-1)\n",
        "        \n",
        "        # Apply SwiGLU: SiLU(gate) * up\n",
        "        hidden = F.silu(gate) * up\n",
        "        \n",
        "        # Apply dropout and final projection\n",
        "        return self.fc2(self.dropout(hidden))\n",
        "\n",
        "\n",
        "# For comparison, let's also implement a standard GELU FFN\n",
        "class FeedForwardGELU(nn.Module):\n",
        "    \"\"\"Standard Transformer FFN with GELU activation\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff, bias=False)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.dropout(F.gelu(self.fc1(x))))\n",
        "\n",
        "\n",
        "# Compare parameter counts\n",
        "d_model = 512\n",
        "d_ff = 1365  # Reduced for SwiGLU to keep params similar to 2048 GELU\n",
        "\n",
        "swiglu_ffn = FeedForwardSwiGLU(d_model, d_ff)\n",
        "gelu_ffn = FeedForwardGELU(d_model, 2048)  # Standard size for GELU\n",
        "\n",
        "swiglu_params = sum(p.numel() for p in swiglu_ffn.parameters())\n",
        "gelu_params = sum(p.numel() for p in gelu_ffn.parameters())\n",
        "\n",
        "print(\"📊 Feed-Forward Layer Comparison\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"SwiGLU FFN (d_ff={d_ff}):     {swiglu_params:,} parameters\")\n",
        "print(f\"GELU FFN (d_ff=2048):         {gelu_params:,} parameters\")\n",
        "print(f\"Parameter ratio:              {swiglu_params/gelu_params:.3f}x\")\n",
        "\n",
        "# Test with sample input\n",
        "batch_size, seq_len = 4, 64\n",
        "test_input = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "print(f\"\\n🧪 Testing with input shape: {test_input.shape}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    swiglu_out = swiglu_ffn(test_input)\n",
        "    gelu_out = gelu_ffn(test_input)\n",
        "\n",
        "print(f\"SwiGLU output shape: {swiglu_out.shape}\")\n",
        "print(f\"GELU output shape:   {gelu_out.shape}\")\n",
        "\n",
        "print(\"\\n✅ Both feed-forward layers working correctly!\")\n",
        "print(\"\\n📝 Note: SwiGLU uses 2/3 the hidden dimension to keep parameter count similar\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🎯 Complete Tiny Transformer with SwiGLU\n",
        "\n",
        "Let's build a complete (but tiny) language model using SwiGLU, similar to modern LLMs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TinyTransformerBlock(nn.Module):\n",
        "    \"\"\"A single transformer block with SwiGLU\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Layer norms (pre-norm style like modern LLMs)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        \n",
        "        # Multi-head attention\n",
        "        self.self_attn = nn.MultiheadAttention(\n",
        "            d_model, n_heads, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        \n",
        "        # SwiGLU feed-forward\n",
        "        self.ffn = FeedForwardSwiGLU(d_model, d_ff, dropout)\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        # Pre-norm attention\n",
        "        norm_x = self.ln1(x)\n",
        "        attn_out, _ = self.self_attn(norm_x, norm_x, norm_x, attn_mask=mask)\n",
        "        x = x + attn_out  # Residual connection\n",
        "        \n",
        "        # Pre-norm feed-forward\n",
        "        norm_x = self.ln2(x)\n",
        "        ffn_out = self.ffn(norm_x)\n",
        "        x = x + ffn_out  # Residual connection\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class TinyTransformer(nn.Module):\n",
        "    \"\"\"A tiny transformer language model (~1M parameters)\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size: int = 500, d_model: int = 128, \n",
        "                 n_layers: int = 2, n_heads: int = 4, d_ff: int = 256):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # Token and position embeddings\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(512, d_model) * 0.02)\n",
        "        \n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TinyTransformerBlock(d_model, n_heads, d_ff)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        \n",
        "        # Final layer norm and language model head\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        \n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "        \n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize weights following modern practices\"\"\"\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "    \n",
        "    def forward(self, input_ids):\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        \n",
        "        # Get embeddings\n",
        "        token_emb = self.token_emb(input_ids)  # (batch, seq, d_model)\n",
        "        pos_emb = self.pos_emb[:seq_len]       # (seq, d_model)\n",
        "        x = token_emb + pos_emb\n",
        "        \n",
        "        # Pass through transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        \n",
        "        # Final layer norm and projection to vocabulary\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)  # (batch, seq, vocab_size)\n",
        "        \n",
        "        return logits\n",
        "\n",
        "\n",
        "# Create and test the model\n",
        "tiny_model = TinyTransformer()\n",
        "total_params = sum(p.numel() for p in tiny_model.parameters())\n",
        "\n",
        "print(\"🤖 Tiny Transformer with SwiGLU\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Model parameters: {total_params:,}\")\n",
        "print(f\"Architecture: 2 layers, 4 heads, 128 d_model\")\n",
        "\n",
        "# Test with dummy input\n",
        "batch_size, seq_len = 2, 32\n",
        "input_ids = torch.randint(0, 500, (batch_size, seq_len))\n",
        "\n",
        "print(f\"\\n🧪 Testing with input shape: {input_ids.shape}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = tiny_model(input_ids)\n",
        "\n",
        "print(f\"Output logits shape: {logits.shape}\")\n",
        "print(f\"Logits represent predictions for 500 vocabulary tokens\")\n",
        "\n",
        "# Show sample predictions for the last token\n",
        "probs = F.softmax(logits[0, -1], dim=-1)\n",
        "top_k = 5\n",
        "top_probs, top_indices = probs.topk(top_k)\n",
        "\n",
        "print(f\"\\n📊 Top {top_k} predicted tokens (for last position):\")\n",
        "for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
        "    print(f\"  {i+1}. Token {idx.item():3d}: {prob.item():.4f} probability\")\n",
        "\n",
        "print(\"\\n✅ Tiny transformer with SwiGLU working correctly!\")\n",
        "print(\"🎉 This architecture is similar to modern LLMs like LLaMA!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🔬 Training Impact Experiment *(Optional)*\n",
        "\n",
        "Let's run a quick training experiment to see SwiGLU vs GELU in action:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create both SwiGLU and GELU versions for comparison\n",
        "class TinyTransformerGELU(TinyTransformer):\n",
        "    \"\"\"Same as TinyTransformer but with GELU instead of SwiGLU\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size: int = 500, d_model: int = 128, \n",
        "                 n_layers: int = 2, n_heads: int = 4, d_ff: int = 384):  # Larger d_ff for GELU\n",
        "        super().__init__(vocab_size, d_model, n_layers, n_heads, d_ff)\n",
        "        \n",
        "        # Replace SwiGLU with GELU in all blocks\n",
        "        for block in self.blocks:\n",
        "            block.ffn = FeedForwardGELU(d_model, d_ff)\n",
        "\n",
        "\n",
        "def train_model_steps(model, num_steps=100, lr=3e-4):\n",
        "    \"\"\"Simple training loop for demonstration\"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_curve = []\n",
        "    \n",
        "    model.train()\n",
        "    for step in range(num_steps):\n",
        "        # Generate random batch (next token prediction task)\n",
        "        batch_size, seq_len = 8, 16\n",
        "        input_ids = torch.randint(0, 500, (batch_size, seq_len))\n",
        "        \n",
        "        # Forward pass\n",
        "        logits = model(input_ids)\n",
        "        \n",
        "        # Next token prediction loss\n",
        "        # Predict tokens 1 to seq_len based on tokens 0 to seq_len-1\n",
        "        loss = F.cross_entropy(\n",
        "            logits[:, :-1].reshape(-1, 500),  # predictions\n",
        "            input_ids[:, 1:].reshape(-1)      # targets\n",
        "        )\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_curve.append(loss.item())\n",
        "        \n",
        "        if step % 25 == 0:\n",
        "            print(f\"Step {step:3d}: Loss = {loss.item():.4f}\")\n",
        "    \n",
        "    return loss_curve\n",
        "\n",
        "\n",
        "# Quick training comparison\n",
        "print(\"🏁 Training Comparison: SwiGLU vs GELU\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create models with similar parameter counts\n",
        "swiglu_model = TinyTransformer(d_ff=256)  # SwiGLU with smaller d_ff\n",
        "gelu_model = TinyTransformerGELU(d_ff=384)  # GELU with larger d_ff\n",
        "\n",
        "swiglu_params = sum(p.numel() for p in swiglu_model.parameters())\n",
        "gelu_params = sum(p.numel() for p in gelu_model.parameters())\n",
        "\n",
        "print(f\"SwiGLU model: {swiglu_params:,} parameters\")\n",
        "print(f\"GELU model:   {gelu_params:,} parameters\")\n",
        "print(f\"Parameter ratio: {swiglu_params/gelu_params:.3f}\")\n",
        "\n",
        "# Train both models\n",
        "print(f\"\\n🚀 Training SwiGLU model...\")\n",
        "swiglu_losses = train_model_steps(swiglu_model, num_steps=100)\n",
        "\n",
        "print(f\"\\n🚀 Training GELU model...\")\n",
        "gelu_losses = train_model_steps(gelu_model, num_steps=100)\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(swiglu_losses, label='SwiGLU', linewidth=2, color='blue')\n",
        "plt.plot(gelu_losses, label='GELU', linewidth=2, color='red')\n",
        "plt.title('Training Loss Comparison', fontweight='bold')\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Cross-Entropy Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# Smooth the curves for better visualization\n",
        "window = 10\n",
        "swiglu_smooth = np.convolve(swiglu_losses, np.ones(window)/window, mode='valid')\n",
        "gelu_smooth = np.convolve(gelu_losses, np.ones(window)/window, mode='valid')\n",
        "\n",
        "plt.plot(swiglu_smooth, label='SwiGLU (smoothed)', linewidth=2, color='blue')\n",
        "plt.plot(gelu_smooth, label='GELU (smoothed)', linewidth=2, color='red')\n",
        "plt.title('Smoothed Training Curves', fontweight='bold')\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Cross-Entropy Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n📊 Final Results:\")\n",
        "print(f\"SwiGLU final loss: {swiglu_losses[-1]:.4f}\")\n",
        "print(f\"GELU final loss:   {gelu_losses[-1]:.4f}\")\n",
        "print(f\"Improvement:       {((gelu_losses[-1] - swiglu_losses[-1]) / gelu_losses[-1] * 100):+.2f}%\")\n",
        "\n",
        "print(\"\\n💡 Note: Results may vary due to random initialization!\")\n",
        "print(\"📚 In real experiments, SwiGLU typically shows consistent improvements\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🎯 Key Takeaways and Real-World Impact\n",
        "\n",
        "Congratulations! You've just built and experimented with SwiGLU from scratch. Let's recap what makes it the champion of modern LLMs:\n",
        "\n",
        "### 🏆 Why SwiGLU Dominates\n",
        "\n",
        "1. **Smooth Gating + Linear Paths** → Better gradient flow than ReLU/GELU alone\n",
        "2. **Dynamic Control** → Network learns context-dependent information flow  \n",
        "3. **No Dead Neurons** → Small negative values can still contribute\n",
        "4. **Proven Results** → Consistent improvements across benchmarks\n",
        "\n",
        "### 📊 The Numbers Don't Lie\n",
        "\n",
        "From real research papers:\n",
        "- **SwiGLU**: 1.944 validation perplexity (Shazeer, 2020)\n",
        "- **ReLU**: 1.997 validation perplexity  \n",
        "- **GELU**: 1.983 validation perplexity\n",
        "- **GeGLU**: 1.942 validation perplexity (slightly better!)\n",
        "\n",
        "That ~3% improvement in perplexity translates to noticeably better language modeling!\n",
        "\n",
        "### 🚀 Where You'll See SwiGLU\n",
        "\n",
        "- **Meta's LLaMA**: \"We replace ReLU with SwiGLU to improve performance\"\n",
        "- **Google's PaLM**: \"SwiGLU significantly increases quality\"  \n",
        "- **Modern Open-Source Models**: Qwen, DeepSeek, Mistral, and many others\n",
        "\n",
        "### 💡 Implementation Tips\n",
        "\n",
        "1. **Extra Matrix Multiply**: SwiGLU uses 3 weight matrices instead of 2\n",
        "2. **Dimension Scaling**: Reduce hidden dimension by ~2/3 to keep compute similar  \n",
        "3. **Parameter Budget**: `d_ff_swiglu ≈ (2/3) * d_ff_gelu` for fair comparison\n",
        "4. **Memory**: Slightly higher memory usage due to extra parameters\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🔬 Exercises for Further Exploration\n",
        "\n",
        "Ready to dig deeper? Try these challenges:\n",
        "\n",
        "### 🟢 Beginner Exercises\n",
        "1. **Activation Variants**: Replace `F.silu` with `F.gelu` in SwiGLU and compare outputs\n",
        "2. **Dimension Experiments**: What happens if you don't reduce the hidden dimension for SwiGLU?\n",
        "3. **Visualization**: Create a heatmap showing how SwiGLU gates different input patterns\n",
        "\n",
        "### 🟡 Intermediate Challenges  \n",
        "4. **Custom GLU**: Implement your own GLU variant using a different activation function (e.g., Mish, ELU)\n",
        "5. **Memory Profiling**: Compare memory usage between SwiGLU and GELU during training\n",
        "6. **Gradient Analysis**: Plot gradient magnitudes through deep networks with different activations\n",
        "\n",
        "### 🔴 Advanced Projects\n",
        "7. **Benchmark Suite**: Implement a comprehensive benchmark comparing all GLU variants\n",
        "8. **Architecture Search**: Experiment with mixing different activations in different layers\n",
        "9. **Scale Analysis**: How do the benefits of SwiGLU change as you scale model size?\n",
        "\n",
        "---\n",
        "\n",
        "## 📚 Further Reading and Resources\n",
        "\n",
        "Want to learn more? Dive into these essential resources:\n",
        "\n",
        "### 📄 Essential Papers\n",
        "- **Shazeer (2020)**: \"GLU Variants Improve Transformer\" - The paper that put SwiGLU on the map\n",
        "- **Dauphin et al. (2016)**: \"Language Modeling with Gated Convolutional Networks\" - Original GLU paper\n",
        "- **Touvron et al. (2023)**: \"LLaMA: Open and Efficient Foundation Language Models\" - How Meta uses SwiGLU\n",
        "- **Chowdhery et al. (2022)**: \"PaLM: Scaling Language Modeling with Pathways\" - Google's experience with SwiGLU\n",
        "\n",
        "### 🌐 Online Resources\n",
        "- **[Hugging Face Transformers](https://github.com/huggingface/transformers)**: Check out LLaMA implementations\n",
        "- **[Andrej Karpathy's minGPT](https://github.com/karpathy/minGPT)**: Simple, educational GPT implementations\n",
        "- **[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)**: Visual guide to transformer architecture\n",
        "\n",
        "### 💻 Code Repositories\n",
        "- **[LLaMA Implementation](https://github.com/facebookresearch/llama)**: Official Meta implementation\n",
        "- **[nanoGPT](https://github.com/karpathy/nanoGPT)**: Minimal, hackable GPT implementation \n",
        "- **[Transformers from Scratch](https://peterbloem.nl/blog/transformers)**: Educational transformer implementations\n",
        "\n",
        "---\n",
        "\n",
        "## 🎉 Conclusion\n",
        "\n",
        "**Amazing work!** You've now mastered one of the key innovations that makes modern LLMs so powerful. SwiGLU might seem like a small change, but it's these incremental improvements that have helped push language models to their current impressive capabilities.\n",
        "\n",
        "The next time you use ChatGPT, Claude, or any modern LLM, remember that deep inside those transformer layers, SwiGLU activations are working their magic—gating information flow and enabling the smooth gradients that make training at scale possible!\n",
        "\n",
        "### What's Next?\n",
        "- **Chapter 2**: Traditional Language Models - See how we got from n-grams to neural networks\n",
        "- **Chapter 3**: Tokenization - Understanding how text becomes numbers\n",
        "- **Chapter 4**: Embeddings - The vector representations that power language understanding\n",
        "- **Chapter 5**: Transformer Architecture - The full picture of modern language models\n",
        "\n",
        "**Keep exploring, keep building, and most importantly—keep having fun with AI! 🚀**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
