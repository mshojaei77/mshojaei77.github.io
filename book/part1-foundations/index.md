---
title: "Part I: Foundations"
nav_order: 1
parent: "LLMs: From Foundation to Production"
has_children: true
description: "Part I covers the mathematical and computational foundations of Large Language Models, from neural networks and embeddings to the complete transformer architecture."
keywords: "Neural Networks, Language Models, Tokenization, Embeddings, Transformer Architecture, Deep Learning Foundations"
---

# Part I: Foundations
{: .no_toc }

Master the mathematical and computational foundations that power Large Language Models.
{: .fs-6 .fw-300 }

---

## ðŸŽ¯ Learning Objectives

By the end of Part I, you will understand the theory and practice of:
- Neural network fundamentals and their role in LLMs.
- The evolution from traditional language models to transformers.
- Tokenization techniques and their impact on model performance.
- Word embeddings and their vector-space properties.
- The complete transformer architecture, including attention mechanisms.

---

## ðŸ“– Chapters

| Chapter | Title | Core Concepts |
|:--------|:------|:--------------|
| 1 | [Neural Networks](01_neural_networks.html) | MLPs, Backpropagation, Optimization |
| 2 | [Traditional Language Models](02_traditional_language_models.html) | N-grams, Perplexity, Smoothing |
| 3 | [Tokenization](03_tokenization.html) | BPE, WordPiece, SentencePiece |
| 4 | [Embeddings](04_embeddings.html) | Word2Vec, GloVe, Vector Arithmetic |
| 5 | [Transformer Architecture](05_transformer_architecture.html) | Self-Attention, Positional Encodings, Encoders/Decoders |

---

*Ready to begin? Start with [Chapter 1: Neural Networks](01_neural_networks.html)* 