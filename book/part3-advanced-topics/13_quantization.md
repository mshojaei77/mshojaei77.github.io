---
title: "Quantization"
nav_order: 13
parent: "Part III: Advanced Topics & Specialization"
grand_parent: "LLMs: From Foundation to Production"
description: "A comprehensive look at model quantization, the process of reducing the precision of an LLM's weights to make it smaller and faster, covering GPTQ, AWQ, and the GGUF format."
keywords: "Quantization, GPTQ, AWQ, GGUF, llama.cpp, 4-bit, 8-bit, Post-Training Quantization, Quantization-Aware Training"
---

# 13. Quantization
{: .no_toc }

**Difficulty:** Intermediate | **Prerequisites:** Model Optimization
{: .fs-6 .fw-300 }

Large Language Models are... large. Quantization is the process of reducing the numerical precision of a model's weights (e.g., from 16-bit to 4-bit numbers), which dramatically reduces its memory footprint and often makes it run faster, enabling huge models to run on consumer hardware.

---

## üìö Core Concepts

<div class="concept-grid">
  <div class="concept-grid-item">
    <h4>Quantization Fundamentals</h4>
    <p>The core idea of mapping high-precision floating-point numbers to lower-precision integers, and the trade-offs between model size and accuracy.</p>
  </div>
  <div class="concept-grid-item">
    <h4>Post-Training Quantization (PTQ)</h4>
    <p>The most common approach, where a fully trained model is quantized after the fact, often with a small calibration dataset.</p>
  </div>
  <div class="concept-grid-item">
    <h4>Quantization-Aware Training (QAT)</h4>
    <p>A more complex method where the quantization process is simulated during training, which can lead to better performance for the quantized model.</p>
  </div>
  <div class="concept-grid-item">
    <h4>GPTQ & AWQ</h4>
    <p>Advanced PTQ algorithms that are particularly effective for quantizing GPT-style models, taking into account weight and activation importance.</p>
  </div>
  <div class="concept-grid-item">
    <h4>GGUF and llama.cpp</h4>
    <p>A popular file format (GGUF) and runtime (llama.cpp) designed for efficiently running quantized LLMs on a wide variety of hardware, including CPUs.</p>
  </div>
  <div class="concept-grid-item">
    <h4>BitsAndBytes</h4>
    <p>A popular library that provides easy-to-use functions for 4-bit and 8-bit quantization, often used in conjunction with Hugging Face libraries.</p>
  </div>
</div>

---

## üõ†Ô∏è Hands-On Labs

1.  **Quantize a Model with GPTQ**: Use a library like `auto-gptq` to apply post-training quantization to a model and measure the reduction in size and the impact on perplexity.
2.  **Quantize a Model with AWQ**: Repeat the process with `auto-awq` and compare the results to your GPTQ model.
3.  **Run a GGUF Model**: Download a pre-quantized model in GGUF format from the Hugging Face Hub and run it locally using `llama.cpp`.

---

## üß† Further Reading

- **[Frantar et al. (2022), "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"](https://arxiv.org/abs/2210.17323)**: The paper that introduced the GPTQ algorithm.
- **[Lin et al. (2023), "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"](https://arxiv.org/abs/2306.00978)**: The paper introducing AWQ.
- **[The `llama.cpp` GitHub Repository](https://github.com/ggerganov/llama.cpp)**: The home of the llama.cpp project, with extensive documentation and examples.
- **[The `auto-gptq` Library](https://github.com/PanQiWei/AutoGPTQ)**: A popular library for applying the GPTQ algorithm. 