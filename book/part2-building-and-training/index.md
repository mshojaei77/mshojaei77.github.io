---
title: "Part 2: Building & Training Models"
nav_order: 2
has_children: true
permalink: /book/part2-building-and-training/
---

# Part 2: Building & Training Models 🧬

**🎯 Focus:** Data preparation, pre-training, fine-tuning, preference alignment  
**📈 Difficulty:** Intermediate to Advanced  
**🎓 Outcome:** Ability to train and fine-tune language models from scratch

**🎯 Learning Objectives:** Learn to prepare high-quality datasets, implement distributed pre-training, create instruction datasets, perform supervised fine-tuning, and align models with human preferences using advanced techniques like RLHF and DPO.

## Chapters in This Part

6. [Data Preparation](06_data_preparation.md)
7. [Pre-Training Large Language Models](07_pre_training_large_language_models.md)
8. [Post-Training Datasets (for Fine-Tuning)](08_post_training_datasets.md)
9. [Supervised Fine-Tuning (SFT)](09_supervised_fine_tuning.md)
10. [Preference Alignment (RL Fine-Tuning)](10_preference_alignment.md)

---

**Prerequisites:**
- Completion of Part 1: The Foundations
- Understanding of transformers and neural network training
- Familiarity with distributed computing concepts 