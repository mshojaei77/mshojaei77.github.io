---
title: "Part II: Building & Training Models"
nav_order: 2
parent: "LLMs: From Foundation to Production"
has_children: true
description: "Part II focuses on building, training, and fine-tuning large language models, covering data preparation, pre-training, supervised fine-tuning, and preference alignment."
keywords: "Data Preparation, Pre-Training, Supervised Fine-Tuning, Preference Alignment, RLHF, DPO, Model Training"
---

# Part II: Building & Training Models
{: .no_toc }

Learn to build, train, and fine-tune large language models from scratch.
{: .fs-6 .fw-300 }

---

## ðŸŽ¯ Learning Objectives

By the end of Part II, you will be able to:
- Implement data preparation pipelines for large-scale training.
- Understand the objectives and scaling laws of pre-training.
- Apply supervised fine-tuning for specialized, task-specific models.
- Use preference alignment techniques like RLHF and DPO.
- Build and manage an end-to-end model training workflow.

---

## ðŸ“– Chapters

| Chapter | Title | Core Concepts |
|:--------|:------|:--------------|
| 6 | [Data Preparation](06_data_preparation.html) | Sourcing, Cleaning, Deduplication |
| 7 | [Pre-Training LLMs](07_pre_training_large_language_models.html) | Scaling Laws, Causal Language Modeling |
| 8 | [Post-Training Datasets](08_post_training_datasets.html) | Instruction Tuning, Dataset Creation |
| 9 | [Supervised Fine-Tuning](09_supervised_fine_tuning.html) | Transfer Learning, LoRA, PEFT |
| 10 | [Preference Alignment](10_preference_alignment.html) | RLHF, DPO, Reward Modeling |

---

*Ready to build? Start with [Chapter 6: Data Preparation](06_data_preparation.html)* 