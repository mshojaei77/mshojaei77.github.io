---
title: "GRPO"
nav_order: 4
parent: drafts
layout: default
---


# Group Relative Policy Optimization (GRPO)
From Theory to Fine-Tuning Qwen3-0.6B for Advanced Tool Use

## Introduction

Large Language Models (LLMs) can be fine-tuned to utilize tools, such as calling functions through JSON outputs or reasoning through steps in an agentic `Thought ‚Üí Action ‚Üí Observation ‚Üí Answer` (ReAct) style. In this comprehensive guide, we will explore the process of fine-tuning the Qwen/Qwen3-0.6B model on a Colab T4 GPU to instill these behaviors using Group Relative Policy Optimization (GRPO).

This book will explain GRPO from scratch, highlighting its distinctions from Proximal Policy Optimization (PPO), and apply it to train the model for two key behaviors:

*   **OpenAI-style JSON function calling:** The model will learn to output a JSON object to call a function instead of providing a direct answer.
*   **ReAct-style agentic reasoning:** The model will be trained to produce step-by-step thoughts and actions, such as searching or calculating, before arriving at a final answer.

We will cover every aspect of the process, from dataset preparation and the design of reward functions for correct tool use, to the setup and code for GRPO training with the open-source Hugging Face ü§ó TRL library, and finally, the evaluation of the fine-tuned model's tool-using abilities. This step-by-step guide is designed to be beginner-friendly, featuring code examples and detailed explanations at each stage. By the end, you will not only understand *how* to implement GRPO fine-tuning on Qwen3-0.6B but also *why* it is an effective method for aligning LLMs to complex behaviors, even with limited computational resources.

# Part I: The Theoretical Foundations of Modern LLM Alignment

## Chapter 1: The Standard LLM Training Pipeline

Most capable models undergo a three-stage training pipeline, a foundational concept for understanding where techniques like GRPO fit in.

1.  **Pre-training:** This is the initial, resource-intensive step where a model learns the fundamentals of language. By predicting the next token on terabytes of internet data, the model acquires a vast repository of knowledge but is not yet adept at following specific instructions. The result of this stage is a "base model."
2.  **Supervised Fine-Tuning (SFT):** The base model is then trained on high-quality, curated examples of instruction-response pairs. This process teaches the model to act as a helpful assistant, transforming it into a model like ChatGPT or Llama-3-Instruct. SFT is crucial for priming the model to behave like a conversational agent.
3.  **Reinforcement Learning (RL) / Preference Tuning:** This is the final stage of polishing the model. Using feedback, either from human evaluators (RLHF) or automated signals, the model's behavior is further aligned with desired outcomes. Algorithms such as PPO, DPO, and the focus of this book, GRPO, are employed in this phase.

Our focus is entirely on the third stage. GRPO serves as a tool for this critical final alignment step.

### 1.1 The Limits of Supervised Fine-Tuning (SFT)

While SFT is a crucial step, it has inherent limitations. For many tasks, a single "correct" response does not exist. Desirable qualities such as helpfulness, harmlessness, creativity, or the coherence of a multi-step reasoning plan are subjective and difficult to capture within a static dataset. It is impractical, and often impossible, to create a comprehensive SFT dataset that covers every nuance of human preference. This is where Reinforcement Learning from Human Feedback (RLHF) becomes an essential technique.

## Chapter 2: An Introduction to Reinforcement Learning for LLMs

In the context of LLMs, reinforcement learning can be understood in straightforward terms, moving away from traditional robotics analogies.

*   **Agent:** The LLM being trained.
*   **Environment:** Everything external to the agent, including the user, datasets, and any available tools.
*   **State (`s`):** The current context, which for an LLM is the conversation history up to the present turn.
*   **Action (`a`):** The next token generated by the LLM.
*   **Reward (`r`):** A numerical score assigned after the model has generated a complete response, indicating how well it performed.

The process operates as a feedback loop: the model generates text (a sequence of actions), this output is scored (the reward), and the score is used to update the model's parameters (the policy) to encourage better text generation in the future.

A significant challenge in this process is that the reward signal is often sparse and delayed. For instance, the correctness of a mathematical proof can only be determined after the entire proof has been written, not after each individual token.

### 2.1 Reinforcement Learning from Human Feedback (RLHF)

RLHF is a technique that leverages human feedback to optimize a model's behavior, aligning it more closely with human goals and values. It reframes text generation as an RL problem where the LLM is an agent, the conversational context is the environment, and the tokens it generates are its actions. The objective is to train the agent to select actions that maximize a cumulative reward.

The primary challenge lies in defining this reward. Instead of manually programming a reward function, which is not feasible for complex linguistic tasks, RLHF learns it from human preferences. The classic RLHF process, popularized by models like InstructGPT, consists of three phases:

1.  **Supervised Fine-Tuning (SFT):** A pre-trained model is fine-tuned on a dataset of high-quality demonstrations to learn the basic instruction-following format. This SFT model serves as the starting point for the RL phase.
2.  **Reward Model (RM) Training:** This is the "Human Feedback" component. The SFT model generates multiple responses for a given set of prompts. Human labelers then rank these responses from best to worst based on quality. This collection of prompts and ranked responses forms a preference dataset. A separate language model, the Reward Model (RM), is then trained on this dataset to predict the score a human would assign to a given prompt-response pair.
3.  **RL Fine-Tuning:** In the final phase, the SFT model (now the policy model) is further fine-tuned using an RL algorithm. For each training step, a prompt is sampled, and the policy model generates a response. The Reward Model evaluates this response and returns a reward score. The RL algorithm uses this signal to update the policy model's weights, encouraging the generation of responses that will receive higher scores from the RM.

This pipeline is a powerful method for scaling human supervision. The Reward Model acts as a learned, automated proxy for a human judge, enabling the policy model to be optimized over a vast space of possible generations.

### 2.2 The Challenge of Reinforce and the Need for a Baseline

To understand the mechanics of RL algorithms, we can look at a fundamental method called REINFORCE. In this method, the model's parameters are updated based on the generated action and the resulting reward. An action that leads to a high reward is reinforced, increasing its likelihood in the future.

The update rule can be simplified to the following concept: the change in the model's parameters is proportional to the gradient of the log probability of the action, multiplied by the reward.

`ŒîŒ∏ ‚àù ‚àáŒ∏ log(œÄ(action|state)) * Reward`

However, using the raw reward can lead to high variance in the training process. To stabilize this, a baseline is subtracted from the reward. This baseline is a function of the state and does not depend on the action taken. The resulting value is called the **advantage**.

`Advantage (A) = Reward (r) - Baseline (b(s))`

The advantage indicates whether an action was better or worse than the average expected outcome for a given state. If the advantage is positive, the action was better than average, and its probability is increased. If it is negative, the action was worse than average, and its probability is decreased. This normalization makes the learning process more stable and efficient.

## Chapter 3: Deconstructing Proximal Policy Optimization (PPO)

For years, the dominant algorithm in the RLHF fine-tuning phase has been Proximal Policy Optimization (PPO). PPO is a robust algorithm that mitigates some of the instability found in earlier policy gradient methods. However, its implementation for RLHF is notoriously complex and resource-intensive.

### 3.1 The Four-Model Orchestra of PPO

A standard PPO-based RLHF training loop involves four distinct models operating in concert, often referred to as an "Actor-Critic" setup.

1.  **The Policy Model (The Actor):** This is the LLM that is being actively trained. It generates text (actions) based on the current state.
2.  **The Critic (The Value Model):** This is a separate model, often of a similar size to the actor, whose sole purpose is to predict the expected future reward from a given state. It is trained to estimate the value function, which serves as a sophisticated baseline for calculating the advantage.
3.  **The Reward Model:** This is the model trained during the second phase of RLHF. It is frozen during this stage and acts as the "judge," providing the scalar reward signal for the responses generated by the policy model.
4.  **The Reference Model:** This is a frozen, read-only copy of the policy model as it was at the start of the RL phase (the SFT model). It serves as a regularization anchor. A KL divergence penalty is calculated between the policy model's output and the reference model's output. This penalty discourages the policy model from deviating too drastically from its original, well-behaved state, a phenomenon known as "reward hacking."

*Figure 1: A schematic comparison of the PPO and GRPO architectures. PPO (top) utilizes a separate value model to estimate the advantage (A) and optimizes the policy with a clipping mechanism. GRPO (bottom) dispenses with the value model and instead computes the advantage from the rewards of a group of sampled outputs (r‚ÇÅ‚Ä¶r_G), normalizing by the group‚Äôs mean and standard deviation. Both employ a KL penalty to maintain proximity to a reference model.*



The primary disadvantage of this system is its immense computational and memory footprint. During training, both the policy model and the value model require backpropagation, meaning their gradients and optimizer states must be stored in VRAM. This effectively doubles the memory required for trainable parameters compared to a standard SFT run, in addition to the overhead of loading the reference and reward models for inference. This systemic complexity makes PPO resource-intensive, slow, and challenging to manage.

### 3.2 The PPO Clipped Surrogate Objective

The core mechanism of PPO is its "clipped surrogate objective." It ensures stability by operating within a "trust region," making only small, incremental improvements to the policy.

It achieves this by first calculating the probability ratio between the new policy (the one being optimized) and the old policy (the one that generated the data) for a given action. This ratio is then multiplied by the advantage estimate. To prevent this product from becoming excessively large, PPO clips the ratio within a small range, typically `[1 - Œµ, 1 + Œµ]`, where Œµ is a hyperparameter like 0.2. The final objective takes the minimum of the unclipped and clipped values, ensuring the policy update does not stray too far from the previous policy in a single step.

This layered conservatism, combined with the multiple-model architecture, makes PPO effective but also contributes to its complexity and high resource demands, which directly motivated the development of more efficient alternatives like GRPO.

## Chapter 4: Group Relative Policy Optimization (GRPO) from First Principles

**Group Relative Policy Optimization (GRPO)** is a reinforcement learning algorithm developed to fine-tune LLMs efficiently, particularly for complex reasoning tasks. It is a variant of PPO with key differences aimed at improving memory efficiency and stability. GRPO was introduced in the paper "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" and was a component in the training of the DeepSeek-R1 model.

In simple terms, GRPO's main innovations are:

*   **No separate value network:** Unlike PPO, GRPO eliminates the value function model (the critic) used to predict future rewards. This significantly reduces memory and computational overhead.
*   **Group-based comparisons:** GRPO evaluates outputs in groups rather than individually. For each input prompt, it generates a group of multiple completions and uses their *relative* performance to compute advantages. The **average reward of the group** serves as a baseline. The advantage for a given output is its reward minus the group's mean reward, often normalized by the group's standard deviation. This allows the model to learn by comparing its own outputs for the same prompt.
*   **KL divergence in the loss:** Similar to PPO, GRPO incorporates a penalty for KL divergence from a reference policy (typically the pre-trained model) directly into its loss function. This ensures that updates are conservative, preventing the model from deviating wildly while optimizing for the reward.

In essence, **GRPO simplifies the RL pipeline** for LLMs. Where PPO-based RLHF often required training four models (policy, reward, value, and a reference copy), GRPO removes the value network. This reduction in the number of models and gradients involved **cuts memory and compute requirements by roughly half** compared to PPO-based methods, making RL fine-tuning feasible on smaller GPUs.

### 4.1 The "Group Relative" Advantage Calculation

The core innovation of GRPO is its method for calculating the advantage without a critic model. Instead of learning a complex baseline, it generates one on the fly.

For a given prompt, the model generates not one, but a *group* of `G` different responses (e.g., `G=4` or `G=8`). The reward is then calculated for each response in the group. The baseline is simply the average reward of that group. The "advantage" for any single response `i` is its Z-score:

`Advantage (A_i) = (Reward (r_i) - mean(r_group)) / std(r_group)`

This approach, outlined in the DeepSeek-Math paper, estimates the baseline from the group's statistics instead of a dedicated critic model. This saves a significant amount of VRAM and simplifies the training process. The objective is to maximize the expected PPO loss, but with this group-relative advantage and a KL-divergence penalty to prevent the model from straying too far from its original instruction-tuned state. The GRPO objective can be approximated as:

`J_GRPO(Œ∏) ‚âà E [ (1/G) * Œ£(min( (œÄ_Œ∏ / œÄ_Œ∏_old) * A_i, clip(...) * A_i ) - Œ≤ * D_KL[œÄ_Œ∏ || œÄ_ref]) ]`

The core update remains a simplified PPO loss, regularized by a KL penalty against a reference model (`œÄ_ref`, usually the SFT model before RL), which keeps it grounded.

### 4.2 Why GRPO Works for Complex Reasoning

The fundamental idea behind GRPO is that by **comparing multiple outputs for the same prompt**, the model receives a more stable learning signal about what constitutes a "better" answer for that specific prompt. Traditional PPO relies on a value estimator to guess the quality of an output, which can be difficult to train for language tasks. GRPO, in contrast, leverages a *relative ranking* approach. If one output in a group receives a higher reward, the model should increase its probability; if another is worse, it should decrease its probability. This aligns well with how preference reward models are often trained (by comparing two outputs for the same query).

By using the group's average as a dynamic baseline, GRPO performs an on-the-fly normalization of rewards. This yields an advantage signal that steers the policy toward better-than-average outputs and away from worse-than-average ones. Despite its simplicity, GRPO has been shown to enhance complex reasoning abilities, as demonstrated by its use in training the state-of-the-art DeepSeek-Math and DeepSeek-R1 models.

# Part II: A Practical Guide to Fine-Tuning Qwen3-0.6B with GRPO for Agentic Behavior

This section provides a complete, hands-on implementation of fine-tuning the Qwen/Qwen3-0.6B model to enhance its function-calling and agentic reasoning capabilities. We will cover environment setup, model preparation using parameter-efficient techniques, designing custom reward functions, curating a suitable dataset, and executing the training pipeline with the Hugging Face TRL library.

## Chapter 5: Environment and Model Preparation

A reproducible and efficient training environment is the cornerstone of any successful fine-tuning project. This involves installing the necessary libraries and loading the model in a memory-conscious manner.

### 5.1 Step 1: Setting up the Colab Environment

First, install the required Python libraries. This includes `transformers`, `peft`, and `datasets` from the Hugging Face ecosystem; `trl` for the GRPOTrainer; `accelerate` for efficient training; and `bitsandbytes` for quantization. The Qwen3 model requires a recent version of `transformers` (v4.51.0 or newer) due to its custom architecture.

```python
# It is recommended to use a library like unsloth for optimized training
# pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# pip install --no-deps transformers peft accelerate bitsandbytes trl datasets

# For a standard setup, use pip:
!pip install transformers accelerate trl datasets "unsloth[colab-new]" bitsandbytes
```

### 5.2 Step 2: Loading the Qwen3-0.6B Model and Tokenizer

We will now load the Qwen3-0.6B model from the Hugging Face Hub. To make this tutorial accessible on consumer-grade GPUs (like a Colab T4 with ~16 GB of VRAM), it is essential to load the model using quantization. We will use 4-bit quantization, which reduces the model's memory footprint significantly with minimal impact on performance. We will also use a library like `unsloth` which provides highly optimized kernels for faster and more memory-efficient training.

```python
import torch
from unsloth import FastLanguageModel

model_id = "Qwen/Qwen3-0.6B"
max_seq_length = 2048 # Controls the context length

# Load the model with 4-bit quantization
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_id,
    max_seq_length = max_seq_length,
    dtype = None, # Will auto-detect torch.bfloat16 if available
    load_in_4bit = True,
)

# Also load a reference model for the KL penalty
ref_model, _ = FastLanguageModel.from_pretrained(
    model_name = model_id,
    max_seq_length = max_seq_length,
    dtype = None,
    load_in_4bit = True,
)
```

**Tokenizer Configuration:**
Before training, it is important to set the tokenizer's padding token if it is not already set. The TRL library requires left-padding for efficient batch generation of sequences with varying lengths. If the tokenizer lacks a pad token, we use the end-of-sentence (EOS) token as the pad token.

```python
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"
```

### 5.3 Configuring LoRA with PEFT

Full fine-tuning, which updates all of a model's parameters, is computationally expensive. **Parameter-Efficient Fine-Tuning (PEFT)** methods offer a solution by freezing the majority of the base model's weights and training only a small number of additional parameters, known as adapters. **Low-Rank Adaptation (LoRA)** is one of the most popular PEFT techniques. It injects trainable, low-rank matrices into specific layers of the transformer architecture, drastically reducing the number of trainable parameters and the memory required for training.

The choice of LoRA hyperparameters is critical. A lower rank (like `r=8`) provides a more constrained update, which is effective for teaching a new skill like function calling without causing the model to forget its pre-trained abilities. The Qwen3 model family is distinguished by its "thinking" mode, where it can generate an explicit chain-of-thought process within `<think>...</think>` tags. A low-rank update helps preserve this delicate, pre-existing reasoning structure.

```python
from peft import LoraConfig

# Configure LoRA
model = FastLanguageModel.get_peft_model(
    model,
    r = 8,
    lora_alpha = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj"],
    lora_dropout = 0.05,
    bias = "none",
    use_gradient_checkpointing = True,
    random_state = 42,
)
```

## Chapter 6: Dataset and Reward Function Design

For GRPO to work, it requires a reward function to score the completions generated by the policy model. The real power of GRPO for agentic tasks lies in using programmatic reward functions‚Äîsimple Python functions that evaluate completions based on a set of verifiable rules.

### 6.1 Step 3: Preparing the Dataset for Tool Use

For this beginner-friendly guide, we will create a small, synthetic dataset to illustrate two behaviors: function calling and ReAct-style reasoning. In a real-world scenario, a much larger dataset of prompts requiring tool use would be prepared.

We will use Hugging Face's `datasets` library to create an in-memory dataset. Each entry will contain a `prompt` string that instructs the model on what to do.

```python
from datasets import Dataset

# Two example prompts:
prompts = [
    "User: What is the weather in London? (Respond with a JSON function call)",
    "User: What is 12 * 3? (Show reasoning with Thought and Action)"
]

# Create the dataset
dataset = Dataset.from_dict({"prompt": prompts})
```

Each prompt includes a parenthetical instruction to guide the model's response format. While in a real application, the model might learn when to use a tool from a system message, these explicit instructions provide a clear training signal for this exercise.

### 6.2 Step 4: Designing the Programmatic Reward Function

This is the core of the RL training process: a function that scores the model's outputs. Our function will accept a list of completions and their corresponding prompts and return a list of reward values.

The reward function will be designed to assess:
*   **Correct Format:** Does the output adhere to the required format (e.g., valid JSON or a "Thought/Action/Answer" structure)?
*   **Correct Content/Result:** Does the output solve the user's query correctly (e.g., calling the right function with the correct parameters or arriving at the correct final answer)?

Here is an implementation with detailed comments:

```python
import json
import re

def reward_tool_use(completions, prompts, **kwargs):
    rewards = []
    for comp, prompt in zip(completions, prompts):
        comp_str = comp.strip()
        # Case 1: JSON function call expected
        if "JSON" in prompt or comp_str.startswith("{"):
            try:
                result = json.loads(comp_str)
            except Exception:
                rewards.append(0.0)
                continue
            if isinstance(result, dict) and "function" in result:
                if re.search(r"weather in (\w+)", prompt, re.IGNORECASE):
                    city = re.search(r"weather in (\w+)", prompt, re.IGNORECASE).group(1)
                    json_str = comp_str.lower()
                    if city.lower() in json_str:
                        rewards.append(1.0) # Correct function and argument
                    else:
                        rewards.append(0.5) # Correct format, wrong argument
                else:
                    rewards.append(0.5) # Correct format, unexpected function
            else:
                rewards.append(0.0)
        # Case 2: ReAct reasoning expected
        elif "Thought:" in comp_str or "Action:" in comp_str:
            has_thought = "Thought:" in comp_str
            has_action = "Action:" in comp_str
            has_answer = "Answer:" in comp_str
            
            format_reward = 1.0 if (has_thought and has_action and has_answer) else 0.0
            correctness_reward = 0.0

            if re.search(r"\d+ *[\+\-\*\/] *\d+", prompt):
                expr = re.search(r"(\d+ *[\+\-\*\/] *\d+)", prompt).group(1)
                try:
                    expected_value = eval(expr)
                except Exception:
                    expected_value = None
                
                match = re.search(r"Answer:\s*([\-\d]+)", comp_str)
                if match:
                    try:
                        final_answer = int(match.group(1))
                    except ValueError:
                        final_answer = None
                else:
                    final_answer = None

                if expected_value is not None and final_answer == expected_value:
                    correctness_reward = 1.0
            
            total_reward = format_reward + correctness_reward
            # Penalize correct format with wrong answer
            if correctness_reward == 0.0:
                total_reward = 0.5 if format_reward == 1.0 else 0.0
            
            rewards.append(total_reward)
        else:
            rewards.append(0.0) # Output did not attempt the required format
            
    return rewards
```

This reward function provides a scalar signal for each model output. During GRPO training, the model will generate multiple completions per prompt. The reward function will produce a list of scores, and GRPO will use their relative values to update the policy. For example, if one completion gets a reward of 1.0 (correct JSON output) and others get 0.0, the one with 1.0 will have a high positive advantage, pushing the model toward that desired output format.

## Chapter 7: The GRPO Training Pipeline

With the data and reward function in place, we can now configure and run the GRPO training loop.

### 7.1 Step 5: Configuring and Running the GRPO Training Loop

We will use the `GRPOTrainer` from the Hugging Face TRL library, which simplifies the training process. The `GRPOConfig` class is used to specify training hyperparameters.

Key configuration settings include:
*   `num_train_epochs`: The number of times to iterate over the dataset.
*   `per_device_train_batch_size`: The number of prompts per batch.
*   `num_generation`: The number of completions to generate for each prompt in the group. A value of 4 or 8 is common.
*   `learning_rate`: The learning rate for the optimizer.
*   `beta`: The coefficient for the KL divergence penalty.

Now, we initialize the trainer:

```python
from trl import GRPOTrainer, GRPOConfig

# Define GRPO training arguments
training_args = GRPOConfig(
    output_dir="./qwen3-0.6b-grpo-agent",
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    optim="adamw_8bit",
    logging_steps=1,
    learning_rate=1e-5,
    remove_unused_columns=False,
    # GRPO-specific arguments
    num_generation=4, # Generate 4 completions per prompt
    max_prompt_length=1024,
    max_completion_length=100,
    beta=0.1, # KL divergence coefficient
    loss_type="bnpo", # Recommended loss type
)

# Initialize the GRPO trainer
trainer = GRPOTrainer(
    model=model,
    ref_model=ref_model, # Pass the reference model
    args=training_args,
    tokenizer=tokenizer,
    train_dataset=dataset,
    peft_config=model.peft_config['default'], # Pass the peft config
    reward_funcs=reward_tool_use, # Pass our custom reward function
)

# Start training
trainer.train()

# Save the final LoRA adapter
trainer.save_model("./qwen3-0.6b-grpo-agent/final_adapter")
```
During training, you will see logs for each step, showing metrics like the average reward, reward standard deviation, and KL divergence. The goal is to see the average reward increase and stabilize near the maximum value as the model learns to consistently produce the desired output format.

# Part III: Finalizing and Evaluating Your Agentic Model

After training, the final steps are to prepare the model for efficient inference and to rigorously evaluate its newly acquired capabilities.

## Chapter 8: Evaluation and Deployment

### 8.1 Step 6: Evaluating the Fine-Tuned Model

To test if our model has learned to use the tools, we will prompt the fine-tuned model with new queries to check its generalization.

```python
# Switch model to evaluation mode
trainer.model.eval()

test_prompts = [
    "User: What is the weather in New York? (Respond with a JSON function call)",
    "User: What is 7 + 5? (Show reasoning with Thought and Action)"
]

for query in test_prompts:
    inputs = tokenizer(query, return_tensors="pt").to(model.device)
    outputs = trainer.model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"Prompt: {query}\nAssistant: {answer.replace(query, '').strip()}\n{'-'*40}")
```

If the training was effective, the model's responses should follow the requested formats. For the weather question, it should return a JSON object with the function and location. For the arithmetic question, it should provide a ReAct-style reasoning trace.

### 8.2 From LoRA Adapter to Deployed Model: Merging and Saving

The output of our training is a set of LoRA adapters. For deployment, it is more efficient to merge these adapter weights directly into the base model's weights, creating a single, unified model.

```python
from peft import PeftModel

# Reload the base model in full precision for merging
base_model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "Qwen/Qwen3-0.6B",
    max_seq_length = max_seq_length,
    dtype = torch.bfloat16,
    load_in_4bit = False,
)

# Load the trained LoRA adapter
merged_model = PeftModel.from_pretrained(base_model, "./qwen3-0.6b-grpo-agent/final_adapter")

# Merge the adapter into the base model
final_model = merged_model.merge_and_unload()

# Save the final, merged model to disk
final_model.save_pretrained("./qwen3-0.6b-grpo-agent-merged")
tokenizer.save_pretrained("./qwen3-0.6b-grpo-agent-merged")
```

This merged model is now ready for efficient inference using standard `transformers` pipelines or deployment frameworks.

## Chapter 9: Advanced Considerations and Debugging

### 9.1 Why RL Works: Shaping Existing Capabilities

Research and practical application have shown that reinforcement learning and supervised fine-tuning are more about shaping the capabilities already present in a pre-trained language model rather than teaching it entirely new ones. In an analysis of the DeepSeek-Math model, it was observed that RL enhances performance on metrics like majority voting (Maj@k) more than on pass rate (Pass@k). This suggests that the improvement comes from boosting the probability of correct responses that are already within the model's potential outputs, rather than from an enhancement of its fundamental abilities. The capabilities to perform these tasks are likely already present in the underlying pre-trained model, and fine-tuning helps to "bring them out" by shaping the output distribution to suppress undesirable paths and promote desirable ones.

### 9.2 Debugging and Fine-Tuning Tips

*   **Monitor Training Metrics:** Keep a close watch on the average reward and KL divergence. A steadily increasing average reward indicates that the model is learning the desired behavior. If the KL divergence starts to increase significantly, the model may be deviating too much from its pre-trained distribution, which could harm its linguistic fluency. The `beta` parameter in `GRPOConfig` can be adjusted to control this.
*   **Reward Hacking:** The model might find loopholes in the reward function. For example, it might learn to output simple, valid JSON to get partial credit without correctly solving the task. If this occurs, refine the reward function to be more nuanced and specific.
*   **Balanced Training:** In a more realistic agentic setting, the model must learn *when* to use a tool versus answering directly. A balanced dataset with a mix of prompts‚Äîsome requiring tools and some not‚Äîcan help the model learn this distinction. The reward function should be designed to penalize unnecessary tool use.
*   **Use SFT as a Kickstart:** It can be beneficial to perform a brief period of supervised fine-tuning on examples of tool-use behaviors before starting RL. This gives the model a good prior on the desired format. The DeepSeek-R1 pipeline, for instance, alternated between SFT and RL stages.
*   **Rigorously Evaluate:** Evaluation is critical for understanding the true capabilities and limitations of the fine-tuned model. For function calling, key metrics include function name accuracy, argument accuracy, and relevance detection accuracy. For agentic behavior, a holdout test set with predefined success criteria should be used to measure task success rate, trajectory efficiency, and error recovery.

## Conclusion

Group Relative Policy Optimization represents a significant advancement in making reinforcement learning for LLM alignment more efficient and accessible. By replacing the resource-intensive value model of PPO with a lightweight, statistical baseline computed from a group of generated responses, GRPO dramatically lowers the computational and memory barriers to entry. This innovation is particularly powerful when combined with programmatic reward functions, which allow models to be fine-tuned for complex, verifiable tasks like code generation, mathematical reasoning, and agentic tool use without the need for expensive human preference annotation or reward model training.

This guide has provided a comprehensive journey from the foundational principles of RLHF to a detailed, step-by-step implementation of GRPO for fine-tuning the Qwen3-0.6B model. By following the outlined process‚Äîfrom environment setup and LoRA configuration to the art of reward engineering and rigorous evaluation‚Äîpractitioners can effectively enhance the agentic capabilities of modern LLMs. The ability to imbue models with reliable function-calling and multi-step reasoning skills is a critical enabler for the next generation of autonomous AI systems, and GRPO provides a powerful, practical, and efficient pathway to achieving that goal.
