# References


## 1️⃣ Core-Prerequisite Warm-ups

```
"Python decorators tutorial for data science"  
"Linear algebra eigenvalues crash course"  
"Probability vs statistics beginner guide"  
"Machine learning fundamentals supervised vs unsupervised"  
"Git branching and merging explained"  
"Linux command line essentials cheat sheet"  
"SQL JOINs visual explanation"  
"Docker container basics for ML engineers"  
```

---

## 2️⃣ Foundations

### Tokenization

```
"Byte Pair Encoding (BPE) algorithm explained"  
"Hugging Face tokenizers custom example"  
"SentencePiece vs WordPiece comparative study"  
"GPT tokenization tiktoken deep dive"  
"Multilingual tokenizer strategies research paper"  
"Token balance analysis script spaCy"  
```

### Embeddings

```
"Word2Vec architecture step by step"  
"SentenceTransformers semantic search tutorial"  
"Fine-tune OpenAI embeddings guide"  
"CLIP multimodal embeddings hands-on"  
"Vector database FAISS vs Pinecone vs Weaviate"  
"Nearest neighbour search HNSW intuitive explanation"  
```

### Neural-Network Basics

```
"Backpropagation numpy from scratch"  
"Activation functions comparison ReLU GELU SiLU"  
"Adam vs AdamW vs SGD optimizer blog"  
"Mixed precision training FP16 vs BF16 pytorch"  
```

### Traditional LMs

```
"N-gram language model smoothing techniques"  
"LSTM vs GRU difference with code"  
"Bidirectional RNN sentiment analysis tutorial"  
"Perplexity metric language modeling explained"  
```

### Transformer Architecture

```
"Self-attention visual explanation 3blue1brown style"  
"Positional encoding sinusoidal vs RoPE"  
"Implement transformer from scratch pytorch"  
"FlashAttention CUDA paper summary"  
"Mixture-of-Experts transformer overview"  
```

---

## 3️⃣ Training Pipeline

### Data Preparation

```
"Web scraping large text datasets python"  
"MinHash deduplication tutorial"  
"PII detection open-source tools"  
"Synthetic text data generation GPT"  
```

### Pre-Training

```
"Masked language modeling objective guide"  
"Distributed data parallel vs pipeline parallel explained"  
"Curriculum learning NLP practical"  
"Scaling laws large language models summary"  
```

### Post-Training Datasets

```
"Instruction tuning dataset creation Alpaca style"  
"Chat template design best practices"  
"Synthetic dialogue generation script"  
```

### Supervised Fine-Tuning (SFT)

```
"LoRA parameter efficient fine tuning walkthrough"  
"QLoRA low-rank adaptation paper breakdown"  
"Domain adaptation fine tune LLM"  
```

### Preference Alignment

```
"RLHF end-to-end example PPO"  
"Direct Preference Optimization (DPO) implementation"  
"Constitutional AI training overview"  
"Reward model training human feedback"  
```

### Model Architecture Variants

```
"Switch Transformer MoE explained"  
"State space models Mamba RWKV intuition"  
"Longformer sliding window attention demo"  
```

### Reasoning & Prompting

```
"Chain of Thought prompting examples"  
"Tree of Thoughts algorithm GitHub"  
"ReAct framework tool use tutorial"  
```

### Evaluation

```
"MMLU benchmark how to run"  
"HumanEval code generation metric"  
"Bias and toxicity detection LLM eval"  
```

---

## 4️⃣ Deployment & Engineering

### Quantization

```
"GPTQ quantization step by step"  
"SmoothQuant paper summary"  
"llama.cpp GGUF guide"  
"INT8 vs INT4 performance benchmark"  
```

### Inference Optimization

```
"vLLM paged attention architecture"  
"KV cache implementation tips"  
"Speculative decoding accelerate inference"  
"Dynamic batching triton server"  
```

### Running LLMs / Apps

```
"FastAPI streaming responses with LLM"  
"LangChain memory-enabled chatbot tutorial"  
"Prompt engineering structured outputs json"  
"Kubernetes GPU auto-scaling LLM inference"  
```

### Retrieval-Augmented Generation (RAG)

```
"Hybrid search BM25 + embeddings explained"  
"LlamaIndex end-to-end RAG pipeline"  
"Graph RAG knowledge graph integration"  
"Self-RAG corrective feedback method"  
```

### Tool Use & Agents

```
"Function calling OpenAI assistants examples"  
"LangGraph multi-agent orchestration guide"  
"AutoGen planner agent tutorial"  
"CrewAI autonomous agents walkthrough"  
```

### Text-to-SQL

```
"Schema linking text to SQL transformer"  
"Few-shot prompting for SQL generation"  
"In-context self-correction SQL LLM"  
```

### Multimodal

```
"CLIP vision language embedding demo"  
"LLaVA visual instruction tuning"  
"Diffusion Transformer (DiT) text to image"  
"Multimodal RAG pdf images audio"  
```

### Model Enhancement

```
"YaRN context window expansion implementation"  
"Knowledge distillation LLM smaller model"  
"Activation-informed model merging (AIM)"  
```

### LLMOps

```
"MLflow model registry for LLMs"  
"Prometheus Grafana token latency dashboards"  
"GitHub Actions CI/CD for Hugging Face models"  
"Cost optimization CUDA spot instances"  
```

### Security & Responsible AI

```
"Prompt injection attack examples and defenses"  
"OWASP LLM Top 10 checklist"  
"Differential privacy in transformer training"  
"Bias detection metrics for language models"  
```



- [LLM Course by Maxime Labonne](https://github.com/mlabonne/llm-course)
- [Follow Maxime Labonne on X](https://twitter.com/maximelabonne)
- [Maxime Labonne on Hugging Face](https://huggingface.co/mlabonne)
- [Maxime Labonne's Blog](https://mlabonne.github.io/blog)
- [LLM Engineer's Handbook](https://packt.link/a/9781836200079)
- [LLM Assistant on HuggingChat](https://hf.co/chat/assistant/66029d2e5f4a884f7aabc9d1)
- [LLM Course Assistant on ChatGPT](https://chat.openai.com/g/g-yviLuLqvI-llm-course)
- [LLM AutoEval - Automatically evaluate your LLMs using RunPod](https://github.com/mlabonne/llm-autoeval)
- [LLM AutoEval Colab Notebook](https://colab.research.google.com/drive/1Igs3WZuXAIv9X0vwqiE90QlEPys8e8Oa?usp=sharing)
- [LazyMergekit - Easily merge models using MergeKit in one click](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing)
- [LazyAxolotl - Fine-tune models in the cloud using Axolotl in one click](https://colab.research.google.com/drive/1TsDKNo2riwVmU55gjuBgB1AXVtRRfRHW?usp=sharing)
- [AutoQuant - Quantize LLMs in GGUF, GPTQ, EXL2, AWQ, and HQQ formats in one click](https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing)
- [Model Family Tree - Visualize the family tree of merged models](https://colab.research.google.com/drive/1s2eQlolcI1VGgDhqWIANfkfKvcKrMyNr?usp=sharing)
- [ZeroSpace - Automatically create a Gradio chat interface using a free ZeroGPU](https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC)
- [AutoAbliteration - Automatically abliteration models with custom datasets](https://colab.research.google.com/drive/1RmLv-pCMBBsQGXQIM8yF-OdCNyoylUR1?usp=sharing)
- [AutoDedup - Automatically deduplicate datasets using the Rensa library](https://colab.research.google.com/drive/1o1nzwXWAa8kdkEJljbJFW1VuI-3VZLUn?usp=sharing)
- [Fine-tune Llama 3.1 with Unsloth - Ultra-efficient supervised fine-tuning in Google Colab](https://mlabonne.github.io/blog/posts/2024-07-29_Finetune_Llama31.html)
- [Fine-tune Llama 3.1 with Unsloth Colab Notebook](https://colab.research.google.com/drive/164cg_O7SV7G8kZr_JXqLd6VC7pd86-1Z?usp=sharing)
- [Fine-tune Llama 3 with ORPO - Cheaper and faster fine-tuning in a single stage with ORPO](https://mlabonne.github.io/blog/posts/2024-04-19_Fine_tune_Llama_3_with_ORPO.html)
- [Fine-tune Llama 3 with ORPO Colab Notebook](https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi)
- [Fine-tune Mistral-7b with DPO - Boost the performance of supervised fine-tuned models with DPO](https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html)
- [Fine-tune Mistral-7b with DPO Colab Notebook](https://colab.research.google.com/drive/15iFBr1xWgztXvhrj5I9fBv20c7CFOPBE?usp=sharing)
- [Fine-tune Mistral-7b with QLoRA Colab Notebook](https://colab.research.google.com/drive/1o_w0KastmEJNVwT5GoqMCciH-18ca5WS?usp=sharing)
- [Fine-tune CodeLlama using Axolotl - End-to-end guide to the state-of-the-art tool for fine-tuning](https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html)
- [Fine-tune CodeLlama using Axolotl Colab Notebook](https://colab.research.google.com/drive/1Xu0BrCB7IShwSWKVcfAfhehwjDrDMH5m?usp=sharing)
- [Fine-tune Llama 2 with QLoRA - Step-by-step guide to supervised fine-tune Llama 2 in Google Colab](https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html)
- [Fine-tune Llama 2 with QLoRA Colab Notebook](https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing)
- [Introduction to Quantization - Large language model optimization using 8-bit quantization](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html)
- [Introduction to Quantization Colab Notebook](https://colab.research.google.com/drive/1DPr4mUQ92Cc-xf4GgAaB6dFcFnWIvqYi?usp=sharing)
- [4-bit Quantization using GPTQ - Quantize your own open-source LLMs to run them on consumer hardware](https://mlabonne.github.io/blog/4bit_quantization/)
- [4-bit Quantization using GPTQ Colab Notebook](https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A?usp=sharing)
- [Quantization with GGUF and llama.cpp - Quantize Llama 2 models with llama.cpp and upload GGUF versions to the HF Hub](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html)
- [Quantization with GGUF and llama.cpp Colab Notebook](https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing)
- [ExLlamaV2: The Fastest Library to Run LLMs - Quantize and run EXL2 models and upload them to the HF Hub](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html)
- [ExLlamaV2: The Fastest Library to Run LLMs Colab Notebook](https://colab.research.google.com/drive/1yrq4XBlxiA0fALtMoT2dwiACVc77PHou?usp=sharing)
- [Merge LLMs with MergeKit - Create your own models easily, no GPU required!](https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit%20copy.html)
- [Merge LLMs with MergeKit Colab Notebook](https://colab.research.google.com/drive/1_JS7JKJAQozD48-LhYdegcuuZ2ddgXfr?usp=sharing)
- [Create MoEs with MergeKit - Combine multiple experts into a single frankenMoE](https://mlabonne.github.io/blog/posts/2024-03-28_Create_Mixture_of_Experts_with_MergeKit.html)
- [Create MoEs with MergeKit Colab Notebook](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing)
- [Uncensor any LLM with abliteration - Fine-tuning without retraining](https://mlabonne.github.io/blog/posts/2024-06-04_Uncensor_any_LLM_with_abliteration.html)
- [Uncensor any LLM with abliteration Colab Notebook](https://colab.research.google.com/drive/1VYm3hOcvCpbGiqKZb141gJwjdmmCcVpR?usp=sharing)
- [Improve ChatGPT with Knowledge Graphs - Augment ChatGPT's answers with knowledge graphs](https://mlabonne.github.io/blog/posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html)
- [Improve ChatGPT with Knowledge Graphs Colab Notebook](https://colab.research.google.com/drive/1mwhOSw9Y9bgEaIFKT4CLi0n18pXRM4cj?usp=sharing)
- [Decoding Strategies in Large Language Models - A guide to text generation from beam search to nucleus sampling](https://mlabonne.github.io/blog/posts/2022-06-07-Decoding_strategies.html)
- [Decoding Strategies in Large Language Models Colab Notebook](https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing)
- [3Blue1Brown - The Essence of Linear Algebra](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)
- [StatQuest with Josh Starmer - Statistics Fundamentals](https://www.youtube.com/watch?v=qBigTkBLU6g&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9)
- [AP Statistics Intuition by Ms Aerin](https://automata88.medium.com/list/cacc224d5e7d)
- [Immersive Linear Algebra](https://immersivemath.com/ila/learnmore.html)
- [Khan Academy - Linear Algebra](https://www.khanacademy.org/math/linear-algebra)
- [Khan Academy - Calculus](https://www.khanacademy.org/math/calculus-1)
- [Khan Academy - Probability and Statistics](https://www.khanacademy.org/math/statistics-probability)
- [Real Python](https://realpython.com/)
- [freeCodeCamp - Learn Python](https://www.youtube.com/watch?v=rfscVS0vtbw)
- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)
- [freeCodeCamp - Machine Learning for Everybody](https://youtu.be/i_LwzRVP7bg)
- [Udacity - Intro to Machine Learning](https://www.udacity.com/course/intro-to-machine-learning--ud120)
- [3Blue1Brown - But what is a Neural Network?](https://www.youtube.com/watch?v=aircAruvnKk)
- [freeCodeCamp - Deep Learning Crash Course](https://www.youtube.com/watch?v=VyWAvY2CF9c)
- [Fast.ai - Practical Deep Learning](https://course.fast.ai/)
- [Patrick Loeber - PyTorch Tutorials](https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4)
- [Lena Voita - Word Embeddings](https://lena-voita.github.io/nlp_course/word_embeddings.html)
- [RealPython - NLP with spaCy in Python](https://realpython.com/natural-language-processing-spacy-python/)
- [Kaggle - NLP Guide](https://www.kaggle.com/learn-guide/natural-language-processing)
- [Jay Alammar - The Illustration Word2Vec](https://jalammar.github.io/illustrated-word2vec/)
- [Jake Tae - PyTorch RNN from Scratch](https://jaketae.github.io/study/pytorch-rnn/)
- [colah's blog - Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [Visual intro to Transformers by 3Blue1Brown](https://www.youtube.com/watch?v=wjZofJX0v4M)
- [LLM Visualization by Brendan Bycroft](https://bbycroft.net/llm)
- [nanoGPT by Andrej Karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY)
- [Tokenization by Andrej Karpathy](https://www.youtube.com/watch?v=zduSFxRajkE)
- [Attention? Attention! by Lilian Weng](https://lilianweng.github.io/posts/2018-06-24-attention/)
- [Decoding Strategies in LLMs by Maxime Labonne](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html)
- [FineWeb by Penedo et al.](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)
- [RedPajama v2 by Weber et al.](https://www.together.ai/blog/redpajama-data-v2)
- [nanotron by Hugging Face](https://github.com/huggingface/nanotron)
- [Parallel training by Chenyan Xiong](https://www.andrew.cmu.edu/course/11-667/lectures/W10L2%20Scaling%20Up%20Parallel%20Training.pdf)
- [Distributed training by Duan et al.](https://arxiv.org/abs/2407.20018)
- [OLMo 2 by AI2](https://allenai.org/olmo)
- [LLM360](https://www.llm360.ai/)
- [Synthetic Data Generator by Argilla](https://huggingface.co/spaces/argilla/synthetic-data-generator)
- [LLM Datasets by Maxime Labonne](https://github.com/mlabonne/llm-datasets)
- [NeMo-Curator by Nvidia](https://github.com/NVIDIA/NeMo-Curator)
- [Distilabel by Argilla](https://distilabel.argilla.io/dev/sections/pipeline_samples/)
- [Semhash by MinishLab](https://github.com/MinishLab/semhash)
- [Chat Template by Hugging Face](https://huggingface.co/docs/transformers/main/en/chat_templating)
- [Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth by Maxime Labonne](https://huggingface.co/blog/mlabonne/sft-llama3)
- [Axolotl - Documentation by Wing Lian](https://axolotl-ai-cloud.github.io/axolotl/)
- [Mastering LLMs by Hamel Husain](https://parlance-labs.com/education/)
- [LoRA insights by Sebastian Raschka](https://lightning.ai/pages/community/lora-insights/)
- [Direct Preference Optimization](https://arxiv.org/abs/2305.18290)
- [TRL](https://huggingface.co/docs/trl/en/index)
- [verl](https://github.com/volcengine/verl)
- [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF)
- [GRPO](https://arxiv.org/abs/2402.03300)
- [PPO](https://arxiv.org/abs/1707.06347)
- [Illustrating RLHF by Hugging Face](https://huggingface.co/blog/rlhf)
- [LLM Training: RLHF and Its Alternatives by Sebastian Raschka](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives)
- [Preference Tuning LLMs by Hugging Face](https://huggingface.co/blog/pref-tuning)
- [Fine-tune with DPO by Maxime Labonne](https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html)
- [Fine-tune with GRPO by Maxime Labonne](https://huggingface.co/learn/llm-course/en/chapter12/5)
- [DPO Wandb logs by Alexander Vishnevskiy](https://wandb.ai/alexander-vishnevskiy/dpo/reports/TRL-Original-DPO--Vmlldzo1NjI4MTc4)
- [Evaluation guidebook by Clémentine Fourrier](https://github.com/huggingface/evaluation-guidebook)
- [Open LLM Leaderboard by Hugging Face](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)
- [Language Model Evaluation Harness by EleutherAI](https://github.com/EleutherAI/lm-evaluation-harness)
- [Lighteval by Hugging Face](https://github.com/huggingface/lighteval)
- [Chatbot Arena by LMSYS](https://lmarena.ai/)
- [Introduction to quantization by Maxime Labonne](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html)
- [Quantize Llama models with llama.cpp by Maxime Labonne](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html)
- [4-bit LLM Quantization with GPTQ by Maxime Labonne](https://mlabonne.github.io/blog/posts/4_bit_Quantization_with_GPTQ.html)
- [Understanding Activation-Aware Weight Quantization by FriendliAI](https://medium.com/friendliai/understanding-activation-aware-weight-quantization-awq-boosting-inference-serving-efficiency-in-10bb0faf63a8)
- [SmoothQuant on Llama 2 7B by MIT HAN Lab](https://github.com/mit-han-lab/smoothquant/blob/main/examples/smoothquant_llama_demo.ipynb)
- [DeepSpeed Model Compression by DeepSpeed](https://www.deepspeed.ai/tutorials/model-compression/)
- [llama.cpp](https://github.com/ggerganov/llama.cpp)
- [GPTQ](https://arxiv.org/abs/2210.17323)
- [EXL2](https://github.com/turboderp/exllamav2)
- [AWQ](https://arxiv.org/abs/2306.00978)
- [mergekit](https://github.com/cg123/mergekit)
- [DARE](https://arxiv.org/abs/2311.03099)
- [TIES](https://arxiv.org/abs/2311.03099)
- [CLIP](https://openai.com/research/clip)
- [Stable Diffusion](https://stability.ai/stable-image)
- [LLaVA](https://llava-vl.github.io/)
- [Merge LLMs with mergekit by Maxime Labonne](https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit.html)
- [Smol Vision by Merve Noyan](https://github.com/merveenoyan/smol-vision)
- [Large Multimodal Models by Chip Huyen](https://huyenchip.com/2023/10/10/multimodal.html)
- [Unsensor any LLM with abliteration by Maxime Labonne](https://huggingface.co/blog/mlabonne/abliteration)
- [Intuitive Explanation of SAEs by Adam Karvonen](https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html)
- [Scaling test-time compute by Beeching et al.](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute)
- [OpenAI](https://platform.openai.com/)
- [Google](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview)
- [Anthropic](https://docs.anthropic.com/claude/reference/getting-started-with-the-api)
- [OpenRouter](https://openrouter.ai/)
- [Hugging Face](https://huggingface.co/inference-api)
- [Together AI](https://www.together.ai/)
- [Hugging Face Hub](https://huggingface.co/models)
- [Hugging Face Spaces](https://huggingface.co/spaces)
- [LM Studio](https://lmstudio.ai/)
- [ollama](https://ollama.ai/)
- [Outlines](https://github.com/outlines-dev/outlines)
- [Run an LLM locally with LM Studio by Nisha Arya](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio)
- [Prompt engineering guide by DAIR.AI](https://www.promptingguide.ai/)
- [Outlines - Quickstart](https://dottxt-ai.github.io/outlines/latest/quickstart/)
- [LMQL - Overview](https://lmql.ai/docs/language/overview.html)
- [Chroma](https://www.trychroma.com/)
- [Pinecone](https://www.pinecone.io/)
- [Milvus](https://milvus.io/)
- [FAISS](https://faiss.ai/)
- [Annoy](https://github.com/spotify/annoy)
- [LangChain - Text splitters](https://python.langchain.com/docs/how_to/#text-splitters)
- [Sentence Transformers library](https://www.sbert.net/)
- [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
- [The Top 7 Vector Databases by Moez Ali](https://www.datacamp.com/blog/the-top-5-vector-databases)
- [LangChain](https://python.langchain.com/docs/get_started/introduction)
- [LlamaIndex](https://docs.llamaindex.ai/en/stable/)
- [Ragas](https://github.com/explodinggradients/ragas/tree/main)
- [DeepEval](https://github.com/confident-ai/deepeval)
- [Llamaindex - High-level concepts](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html)
- [Model Context Protocol](https://modelcontextprotocol.io/introduction)
- [Pinecone - Retrieval Augmentation](https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/)
- [LangChain - Q&A with RAG](https://python.langchain.com/docs/tutorials/rag/)
- [LangChain - Memory types](https://python.langchain.com/docs/how_to/chatbots_memory/)
- [RAG pipeline - Metrics](https://docs.ragas.io/en/stable/concepts/metrics/index.html)
- [RAG-fusion](https://github.com/Raudaschl/rag-fusion)
- [DSPy](https://github.com/stanfordnlp/dspy)
- [LangChain - Query Construction](https://blog.langchain.dev/query-construction/)
- [LangChain - SQL](https://python.langchain.com/docs/tutorials/sql_qa/)
- [Pinecone - LLM agents](https://www.pinecone.io/learn/series/langchain/langchain-agents/)
- [LLM Powered Autonomous Agents by Lilian Weng](https://lilianweng.github.io/posts/2023-06-23-agent/)
- [LangChain - OpenAI's RAG](https://blog.langchain.dev/applying-openai-rag/)
- [DSPy in 8 Steps](https://dspy-docs.vercel.app/docs/building-blocks/solving_your_task)
- [LangGraph](https://www.langchain.com/langgraph)
- [LlamaIndex](https://docs.llamaindex.ai/en/stable/use_cases/agents/)
- [smolagents](https://github.com/huggingface/smolagents)
- [CrewAI](https://docs.crewai.com/introduction)
- [AutoGen](https://github.com/microsoft/autogen)
- [OpenAI Agents SDK](https://github.com/openai/openai-agents-python)
- [Agents Course](https://huggingface.co/learn/agents-course/unit0/introduction)
- [AI Agents Comparison by Jannik Maierhöfer](https://langfuse.com/blog/2025-03-19-ai-agent-comparison)
- [LangGraph](https://langchain-ai.github.io/langgraph/concepts/why-langgraph/)
- [LlamaIndex Agents](https://docs.llamaindex.ai/en/stable/use_cases/agents/)
- [smolagents](https://huggingface.co/docs/smolagents/index)
- [Multi-Query Attention](https://arxiv.org/abs/1911.02150)
- [Grouped-Query Attention](https://arxiv.org/abs/2305.13245)
- [GPU Inference by Hugging Face](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one)
- [LLM Inference by Databricks](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices)
- [Optimizing LLMs for Speed and Memory by Hugging Face](https://huggingface.co/docs/transformers/main/en/llm_tutorial_optimization)
- [Assisted Generation by Hugging Face](https://huggingface.co/blog/assisted-generation)
- [oobabooga](https://github.com/oobabooga/text-generation-webui)
- [kobold.cpp](https://github.com/LostRuins/koboldcpp)
- [Gradio](https://www.gradio.app/)
- [Streamlit](https://docs.streamlit.io/)
- [SkyPilot](https://skypilot.readthedocs.io/en/latest/)
- [TGI](https://github.com/huggingface/text-generation-inference)
- [vLLM](https://github.com/vllm-project/vllm/tree/main)
- [MLC LLM](https://github.com/mlc-ai/mlc-llm)
- [mnn-llm](https://github.com/wangzhaode/mnn-llm/blob/master/README_en.md)
- [Streamlit - Build a basic LLM app](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps)
- [HF LLM Inference Container](https://huggingface.co/blog/sagemaker-huggingface-llm)
- [Philschmid blog by Philipp Schmid](https://www.philschmid.de/)
- [Optimizing latence by Hamel Husain](https://hamel.dev/notes/llm/inference/03_inference.html)
- [garak](https://github.com/leondz/garak/)
- [langfuse](https://github.com/langfuse/langfuse)
- [OWASP LLM Top 10 by HEGO Wiki](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [Prompt Injection Primer by Joseph Thacker](https://github.com/jthack/PIPE)
- [LLM Security by @llm_sec](https://llmsecurity.net/)
- [Red teaming LLMs by Microsoft](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming)
- [DevOps Roadmap](https://github.com/milanm/DevOps-Roadmap)
- [Star History Chart](https://api.star-history.com/svg?repos=mlabonne/llm-course&type=Date)