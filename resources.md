# Learning Resources

## Part 1: LLM Intern 📘

### Prerequisites

**Mathematics & Statistics:**
- [3Blue1Brown - Essence of Linear Algebra](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)
- [StatQuest - Statistics Fundamentals](https://www.youtube.com/watch?v=qBigTkBLU6g&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9)
- [AP Statistics Intuition](https://automata88.medium.com/list/cacc224d5e7d)
- [Immersive Linear Algebra](https://immersivemath.com/ila/learnmore.html)
- [Khan Academy - Linear Algebra](https://www.khanacademy.org/math/linear-algebra)
- [Khan Academy - Calculus](https://www.khanacademy.org/math/calculus-1)
- [Khan Academy - Probability and Statistics](https://www.khanacademy.org/math/statistics-probability)
- [Matrix Calculus Notes](https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf)
- [Review of Differential Calculus](https://web.stanford.edu/class/cs224n/readings/review-differential-calculus.pdf)
- [Derivatives, Backpropagation, and Vectorization](http://cs231n.stanford.edu/handouts/derivatives.pdf)

**Programming & Python:**
- [Real Python](https://realpython.com/)
- [freeCodeCamp - Learn Python](https://www.youtube.com/watch?v=rfscVS0vtbw)
- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)

**Books:**
- [Speech and Language Processing (2024 pre-release)](https://web.stanford.edu/~jurafsky/slp3/)
- [Natural Language Processing](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)
- [A Primer on Neural Network Models for Natural Language Processing](http://u.cs.biu.ac.il/~yogo/nnlp.pdf)
- [Natural Language Processing with PyTorch](http://library.stanford.edu/sfx?genre=book&title=Natural%20language%20processing%20with%20PyTorch)
- [Natural Language Processing with Transformers](https://transformersbook.com/)
- [Generative AI with LangChain](https://amzn.to/3GUlRng)
- [Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)
- [LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)
- [BUILD GPT: HOW AI WORKS](https://www.amazon.com/dp/9152799727)
- [Hands-On Large Language Models](https://www.llm-book.com/)
- [The Chinese Book for Large Language Models](http://aibox.ruc.edu.cn/zws/index.htm)

**Machine Learning Fundamentals:**
- [Machine Learning for Everybody](https://youtu.be/i_LwzRVP7bg)
- [Udacity - Intro to Machine Learning](https://www.udacity.com/course/intro-to-machine-learning--ud120)
- [A Course in Machine Learning](https://web.archive.org/web/20250114002202/http://ciml.info/dl/v0_99/ciml-v0_99-all.pdf)
- [Caltech CS156: Learning from Data](https://www.youtube.com/playlist?list=PLD63A284B7615313A)
- [Stanford CS229: Machine Learning](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)
- [Stanford CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)
- [Oxford Deep NLP](https://github.com/oxford-cs-deepnlp-2017/lectures)
- [Making Friends with Machine Learning](https://www.youtube.com/playlist?list=PLRKtJ4IpxJpDxl0NTvNYQWKCYzHNuy2xG)
- [Applied Machine Learning](https://www.youtube.com/playlist?list=PL2UML_KCiC0UlY7iCQDSiGDMovaupqc83)
- [Introduction to Machine Learning (Tübingen)](https://www.youtube.com/playlist?list=PL05umP7R6ij35ShKLDqccJSDntugY4FQT)
- [Machine Learning Lecture (Stefan Harmeling)](https://www.youtube.com/playlist?list=PLzrCXlf6ypbxS5OYOY3EN_0u2fDuIT6Gt)
- [Statistical Machine Learning (Tübingen)](https://www.youtube.com/playlist?list=PL05umP7R6ij2XCvrRzLokX6EoHWaGA2cC)
- [Probabilistic Machine Learning](https://www.youtube.com/playlist?list=PL05umP7R6ij2YE8rRJSb-olDNbntAQ_Bx)
- [MIT 6.S897: Machine Learning for Healthcare (2019)](https://www.youtube.com/playlist?list=PLUl4u3cNGP60B0PQXVQyGNdCyCTDU1Q5j)
- [Machine Learning with Graphs (Stanford)](https://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn)

**Deep Learning Basics:**
- [3Blue1Brown - But What is a Neural Network?](https://www.youtube.com/watch?v=aircAruvnKk)
- [Deep Learning Crash Course](https://www.youtube.com/watch?v=VyWAvY2CF9c)
- [Fast.ai - Practical Deep Learning](https://course.fast.ai/)
- [Patrick Loeber - PyTorch Tutorials](https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4)
- [Deep Learning Book](https://www.deeplearningbook.org/)
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
- [Introduction to Deep Learning](https://mitpress.mit.edu/books/introduction-deep-learning)
- [Neural Networks: Zero to Hero](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)
- [Andrej Karpathy Series](https://www.youtube.com/@AndrejKarpathy)
- [Umar Jamil Series](https://www.youtube.com/@umarjamilai)
- [Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY)
- [State of GPT](https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2)
- [MIT: Deep Learning for Art, Aesthetics, and Creativity](https://www.youtube.com/playlist?list=PLCpMvp7ftsnIbNwRnQJbDNRqO6qiN3EyH)
- [Stanford CS230: Deep Learning (2018)](https://youtube.com/playlist?list=PLoROMvodv4rOABXSygHTsbvUz4G_YQhOb)
- [Introduction to Deep Learning (MIT)](https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI)
- [CMU Introduction to Deep Learning (11-785)](https://www.youtube.com/playlist?list=PLp-0K3kfddPxRmjgjm0P1WT6H-gTqE8j9)
- [Deep Learning: CS 182](https://www.youtube.com/playlist?list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A)
- [Deep Unsupervised Learning](https://www.youtube.com/playlist?list=PLwRJQ4m4UJjPiJP3691u-qWwPGVKzSlNP)
- [NYU Deep Learning SP21](https://www.youtube.com/playlist?list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI)
- [Foundation Models](https://www.youtube.com/playlist?list=PL9t0xVFP90GD8hox0KipBkJcLX_C3ja67)
- [Deep Learning (Tübingen)](https://www.youtube.com/playlist?list=PL05umP7R6ij3NTWIdtMbfvX7Z-4WEXRqD)
- [Introduction to Deep Learning and Deep Generative Models](https://www.youtube.com/watch?v=1nqCZqDYPp0&list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51)
- [Parallel Computing and Scientific Machine Learning](https://www.youtube.com/playlist?list=PLCAl7tjCwWyGjdzOOnlbGnVNZk0kB8VSa)

### Tokenization

**Core Concepts & Tutorials:**
- [Introduction to Tokenization: A Theoretical Perspective](https://medium.com/@mshojaei77/introduction-to-tokenization-a-theoretical-perspective-b1cc22fe98c5)
- [Understanding BPE Tokenization](https://medium.com/@mshojaei77/understanding-bpe-tokenization-a-hands-on-tutorial-80570314b12f)
- [Fast Tokenizers: How Rust is Turbocharging NLP](https://medium.com/@mshojaei77/fast-tokenizers-how-rust-is-turbocharging-nlp-dd12a1d13fa9)
- [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)
- [minbpe](https://github.com/karpathy/minbpe/blob/master/lecture.md)
- [minbpe](https://www.youtube.com/watch?v=zduSFxRajkE&t=1157s)

**Hands-On Implementations:**
- [Tokenization Techniques](https://colab.research.google.com/drive/1RwrtINbHTPBSRIoW8Zn9BRabxXguRRf0?usp=sharing)
- [GPT Tokenizer Implementation](https://colab.research.google.com/drive/1y0KnCFZvGVf_odSfcNAws6kcDD7HsI0L?usp=sharing)
- [Build and Push a Tokenizer](https://colab.research.google.com/drive/1uYFoxwCKwshkchBgQ4y4z9cDfKRlwZ-e?usp=sharing)
- [Tokenizer Comparison](https://colab.research.google.com/drive/1wVSCBGFm7KjJy-KugYGYETpncWsPgx5N?usp=sharing)
- [Hugging Face Tokenizers](https://colab.research.google.com/drive/1mcFgQ9PX1TFyEAsFOnoS1ozeSz3vM6A1?usp=sharing)
- [New Tokenizer Training](https://colab.research.google.com/drive/1452WFn66MZzYylTNcL6hV5Zd45sskzs7?usp=sharing)

**Interactive Tools:**
- [TikTokenizer](https://tiktokenizer.vercel.app/)
- [Hugging Face Tokenizer Playground](https://huggingface.co/spaces/Xenova/the-tokenizer-playground)
- [OpenAI Tokenizer](https://platform.openai.com/tokenizer)
- [Tokenizer Arena](https://huggingface.co/spaces/Cognitive-Lab/Tokenizer_Arena)

**Libraries & Documentation:**
- [Hugging Face Tokenizers](https://huggingface.co/docs/tokenizers/mastering-tokenizers)
- [SentencePiece](https://github.com/google/sentencepiece)

**Research Papers:**
- [BPE Research Paper](https://arxiv.org/abs/1508.07909)
- [RadarLLM: Cross-Modal Tokenization](https://arxiv.org/abs/2504.09862)
- [CoreMatching: Token-Neuron Synergy](https://arxiv.org/abs/2505.19235)
- [MOM: Memory-Efficient Token Handling](https://arxiv.org/abs/2504.12526)

### Embeddings

**Core Concepts & Tutorials:**
- [Word Embeddings Deep Dive](https://medium.com/@mshojaei77/from-words-to-vectors-a-gentle-introduction-to-word-embeddings-eaadb1654778)
- [Contextual Embedding Guide](https://medium.com/@mshojaei77/beyond-one-word-one-meaning-contextual-embeddings-187b48c6fc27)
- [Sentence Embedding Techniques](https://medium.com/@mshojaei77/beyond-words-mastering-sentence-embeddings-for-semantic-nlp-dc852b1382ba)
- [Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/)
- [The Illustrated BERT, ELMo, and co.](https://jalammar.github.io/illustrated-bert/)
- [CS224N Lecture 1 - Word Vectors](https://www.youtube.com/watch?v=rmVRLeJRkl4)
- [Lena Voita - Word Embeddings](https://lena-voita.github.io/nlp_course/word_embeddings.html)

**Hands-On Implementations:**
- [Interactive Word2Vec Tutorials](https://colab.research.google.com/drive/1dVkCRF0RKWWSP_QQq79LHNYGhead14d0?usp=sharing)
- [Word2vec from Scratch](https://jaketae.github.io/study/word2vec/)
- [Training Sentence Transformers](https://huggingface.co/blog/train-sentence-transformers)
- [Sentence Transformers](https://www.sbert.net/)
- [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)

**Foundational Papers:**
- [BERT Paper](https://arxiv.org/abs/1810.04805)
- [GloVe Paper](https://www.semanticscholar.org/paper/67b692bbfd29c5a30cfd1046efd5f85eecd1ea86)
- [FastText Paper](https://www.semanticscholar.org/paper/d23e59abcae6ba653ba45dcc0ef975438890a3a4)
- [Distributed Representations of Words and Phrases](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)

**Advanced Topics:**
- [Multilingual BERT](https://www.semanticscholar.org/paper/0b0bc70b48aebe608d53a955990cb08f73de5a7d)
- [Bias in Embeddings](https://www.semanticscholar.org/paper/5ea2104a039921633f75a9f4b986b515ddbe96d7)
- [Contextual Word Representations: A Contextual Introduction](https://arxiv.org/abs/1902.06006)
- [Evaluation Methods for Unsupervised Word Embeddings](http://www.aclweb.org/anthology/D15-1036)
- [Improving Distributional Similarity with Lessons Learned from Word Embeddings](http://www.aclweb.org/anthology/Q15-1016)

### Neural Networks

**Core Textbooks & Courses:**
- [Deep Learning Book](https://www.deeplearningbook.org/)
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
- [Stanford CS231n](http://cs231n.stanford.edu/)

**Mathematical Foundations:**
- [CS231n Notes on Network Architectures](http://cs231n.github.io/neural-networks-1/)
- [CS231n Notes on Backpropagation](http://cs231n.github.io/optimization-2/)

**Essential Papers & Articles:**
- [Learning Representations by Backpropagating Errors](http://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf)
- [Yes You Should Understand Backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)
- [Natural Language Processing (Almost) from Scratch](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf)

### Traditional Language Models

**Core Textbooks:**
- [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)
- [Statistical Language Models](https://www.cambridge.org/core/books/statistical-language-models)
- [Deep Learning Book - RNNs](https://www.deeplearningbook.org/contents/rnn.html)

**N-gram Models:**
- [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf)

**RNN & LSTM Resources:**
- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- [Sequence Modeling: Recurrent and Recursive Neural Nets](http://www.deeplearningbook.org/contents/rnn.html)
- [Vanishing Gradients Jupyter Notebook](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lectures/vanishing_grad_example.html)
- [RealPython - NLP with spaCy](https://realpython.com/natural-language-processing-spacy-python/)
- [Kaggle - NLP Guide](https://www.kaggle.com/learn-guide/natural-language-processing)
- [Jake Tae - PyTorch RNN from Scratch](https://jaketae.github.io/study/pytorch-rnn/)

**Foundational Papers:**
- [Neural Probabilistic Language Models](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
- [Empirical Evaluation of Gated RNNs](https://arxiv.org/abs/1412.3555)
- [Learning Long-term Dependencies with Gradient Descent is Difficult](https://ieeexplore.ieee.org/document/279181)
- [On the Difficulty of Training Recurrent Neural Networks](https://arxiv.org/pdf/1211.5063.pdf)

**Historical Context:**
- [On Chomsky and the Two Cultures of Statistical Learning](http://norvig.com/chomsky.html)

**Dependency Parsing:**
- [Dependency Parsing](https://link.springer.com/book/10.1007/978-3-031-02131-2)
- [Jurafsky & Martin Chapter 19](https://web.stanford.edu/~jurafsky/slp3/19.pdf)
- [Incrementality in Deterministic Dependency Parsing](https://www.aclweb.org/anthology/W/W04/W04-0308.pdf)
- [A Fast and Accurate Dependency Parser using Neural Networks](https://www.emnlp2014.org/papers/pdf/EMNLP2014082.pdf)
- [Globally Normalized Transition-Based Neural Networks](https://arxiv.org/pdf/1603.06042.pdf)
- [Universal Stanford Dependencies](http://nlp.stanford.edu/~manning/papers/USD_LREC14_UD_revision.pdf)
- [Universal Dependencies Website](http://universaldependencies.org/)

### Transformers

**Foundational Paper:**
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

**Visual Explanations:**
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [Transformer (Google AI Tutorials)](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)
- [Visual Intro to Transformers](https://www.youtube.com/watch?v=wjZofJX0v4M)
- [LLM Visualization](https://bbycroft.net/llm)
- [nanoGPT](https://www.youtube.com/watch?v=kCc8FmEb1nY)
- [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/)
- [Decoding Strategies in LLMs](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html)
- [Stanford CS25 - Transformers United](https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM)
- [CS25-Transformers United](https://web.stanford.edu/class/cs25/)
- [CS324 - Large Language Models](https://stanford-cs324.github.io/winter2022/)
- [UWaterloo CS 886: Recent Advances on Foundation Models](https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/)
- [Princeton: Understanding Large Language Models](https://www.cs.princeton.edu/courses/archive/fall22/cos597G/)
- [XCS224U: Natural Language Understanding (2023)](https://www.youtube.com/playlist?list=PLoROMvodv4rOwvldxftJTmoR3kRcWkJBp)
- [NLP Course (Hugging Face)](https://www.youtube.com/playlist?list=PLo2EIpI_JMQvWfQndUesu0nPBAtZ9gP1o)
- [CS224N: Natural Language Processing with Deep Learning](https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ)
- [CMU Neural Networks for NLP](https://www.youtube.com/playlist?list=PL8PYTP1V4I8AkaHEJ7lOOrlex-pcxS-XV)
- [CS224U: Natural Language Understanding](https://www.youtube.com/playlist?list=PLoROMvodv4rPt5D0zs3YhbWSZA8Q_DyiJ)
- [CMU Advanced NLP 2021](https://www.youtube.com/playlist?list=PL8PYTP1V4I8AYSXn_GKVgwXVluCT9chJ6)
- [CMU Advanced NLP 2022](https://www.youtube.com/playlist?list=PL8PYTP1V4I8D0UkqW2fEhgLrnlDW9QK7z)
- [CMU Advanced NLP 2024](https://www.youtube.com/playlist?list=PL8PYTP1V4I8DZprnWryM4nR8IZl1ZXDjg)
- [Multilingual NLP 2020](https://www.youtube.com/playlist?list=PL8PYTP1V4I8CHhppU6n1Q9-04m96D9gt5)
- [Multilingual NLP 2022](https://www.youtube.com/playlist?list=PL8PYTP1V4I8BhCpzfdKKdd1OnTfLcyZr7)
- [Advanced NLP](https://www.youtube.com/playlist?list=PLWnsVgP6CzadmQX6qevbar3_vDBioWHJL)

**Technical Deep Dives:**
- [Transformer Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)
- [Understanding Layer Normalization](https://arxiv.org/abs/1607.06450)
- [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf)

**Implementation Tutorials:**
- [PyTorch Transformer Tutorials](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)
- [TensorFlow Transformer Tutorials](https://www.tensorflow.org/tutorials/text/transformer)

**Textbook Resources:**
- [Jurafsky and Martin Chapter 9](https://web.stanford.edu/~jurafsky/slp3/9.pdf)

**Applications & Extensions:**
- [Image Transformer](https://arxiv.org/pdf/1802.05751.pdf)
- [Music Transformer](https://arxiv.org/pdf/1809.04281.pdf)

### Data Preparation

**Data Collection & Scraping:**
- [Common Crawl Documentation](https://commoncrawl.org/the-data/)
- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Scrapy Documentation](https://scrapy.org/)
- [Hugging Face Datasets Guide](https://huggingface.co/docs/datasets/)

**Data Processing Libraries:**
- [Pandas Documentation](https://pandas.pydata.org/)
- [Apache Beam](https://beam.apache.org/)
- [Elasticsearch](https://www.elastic.co/elasticsearch/)

**Data Quality & Ethics:**
- [Data Quality for Machine Learning](https://www.amazon.com/Data-Quality-Machine-Learning-Practices/dp/1492094964)
- [Data Ethics Framework](https://www.gov.uk/government/publications/data-ethics-framework)
- [Responsible AI Practices](https://ai.google/responsibilities/responsible-ai-practices/)

**Text Preprocessing:**
- [Stanford Text Preprocessing](https://nlp.stanford.edu/IR-book/html/htmledition/text-preprocessing-1.html)
- [Google's Data Preparation](https://cloud.google.com/architecture/data-preprocessing-for-ml-with-tf-transform-pt1)

**Version Control & Management:**
- [DVC (Data Version Control)](https://dvc.org/)

**LLM-Specific Resources:**
- [FineWeb](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)
- [RedPajama v2](https://www.together.ai/blog/redpajama-data-v2)
- [NeMo-Curator](https://github.com/NVIDIA/NeMo-Curator)
- [Distilabel](https://distilabel.argilla.io/dev/sections/pipeline_samples/)
- [Semhash](https://github.com/MinishLab/semhash)
- [LLM Datasets](https://github.com/mlabonne/llm-datasets)

## Part 2: LLM Scientist ⚙️

### Pre-Training

**Foundational Understanding:**
- [Understanding Language Models](https://medium.com/@mshojaei77/1ac0e05ca1f3)
- [LLMs Overview](https://arxiv.org/pdf/2307.06435)
- [LLMs Survey](https://arxiv.org/abs/2402.06196)

**Video Resources:**
- [LLMs Explained Briefly](https://www.youtube.com/watch?v=LPZh9BOjkQs)
- [Deep Dive into LLMs](https://www.youtube.com/watch?v=7xTGNNLPyMI)
- [Intro to LLMs](https://www.youtube.com/watch?v=zjkBMFhNj_g)
- [Open Pretrained Transformers](https://www.youtube.com/watch?v=p9IxoSkvZ-M&t=4s)

**Key Research Papers:**
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)

**Training Frameworks & Tools:**
- [DeepSpeed](https://www.deepspeed.ai/)
- [Ray](https://ray.io/)
- [Triton](https://triton-lang.org/)
- [ONNX](https://onnx.ai/)
- [nanotron](https://github.com/huggingface/nanotron)
- [Parallel Training](https://www.andrew.cmu.edu/course/11-667/lectures/W10L2%20Scaling%20Up%20Parallel%20Training.pdf)
- [Distributed Training Survey](https://arxiv.org/abs/2407.20018)
- [OLMo 2](https://allenai.org/olmo)
- [LLM360](https://www.llm360.ai/)

### Post-Training Datasets

**Instruction Datasets:**
- [Alpaca Dataset](https://github.com/tatsu-lab/stanford_alpaca)
- [Dolly Dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k)
- [OpenAssistant Conversations](https://huggingface.co/datasets/OpenAssistant/oasst1)

**Conversation Datasets:**
- [ShareGPT Dataset](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered)

**Preference & RLHF Datasets:**
- [Anthropic HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf)

**Question Answering:**
- [SQuAD](https://arxiv.org/abs/1606.05250)

**Resources:**
- [Synthetic Data Generator](https://huggingface.co/spaces/argilla/synthetic-data-generator)
- [Chat Template](https://huggingface.co/docs/transformers/main/en/chat_templating)

### Supervised Fine-Tuning

**Libraries & Tools:**
- [PEFT Library](https://github.com/huggingface/peft)
- [Unsloth](https://github.com/unslothai/unsloth)
- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)
- [Axolotl Documentation](https://axolotl-ai-cloud.github.io/axolotl/)

**Research Papers:**
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [QLoRA Paper](https://arxiv.org/abs/2305.14314)

**Implementation Examples:**
- [Alpaca Fine-tuning](https://github.com/tatsu-lab/stanford_alpaca)

**Tutorials:**
- [Fine-tune Llama 3.1 with Unsloth](https://mlabonne.github.io/blog/posts/2024-07-29_Finetune_Llama31.html)
- [Fine-tune Llama 3 with ORPO](https://mlabonne.github.io/blog/posts/2024-04-19_Fine_tune_Llama_3_with_ORPO.html)
- [Fine-tune Mistral-7b with DPO](https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html)
- [Fine-tune Mistral-7b with QLoRA](https://colab.research.google.com/drive/1o_w0KastmEJNVwT5GoqMCciH-18ca5WS?usp=sharing)
- [Fine-tune CodeLlama using Axolotl](https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html)
- [Fine-tune Llama 2 with QLoRA](https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html)
- [Mastering LLMs](https://parlance-labs.com/education/)
- [LoRA Insights](https://lightning.ai/pages/community/lora-insights/)
- [ChatGPT Prompt Engineering](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)

**Parameter-Efficient Methods:**
- [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751)
- [The Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635)
- [Few-Shot Learning](https://arxiv.org/abs/2005.14165)
- [Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903)
- [Practical Methodology](https://www.deeplearningbook.org/contents/guidelines.html)

### Preference Alignment

**Libraries & Frameworks:**
- [TRL (Transformer Reinforcement Learning)](https://github.com/huggingface/trl)
- [Ray RLlib](https://docs.ray.io/en/latest/rllib/)

**Core RLHF Papers:**
- [InstructGPT Paper](https://arxiv.org/abs/2203.02155)
- [PPO Paper](https://arxiv.org/abs/1707.06347)
- [DPO Paper](https://arxiv.org/abs/2305.18290)

**Constitutional AI & Safety:**
- [Constitutional AI Paper](https://arxiv.org/abs/2212.08073)
- [RLAIF Paper](https://arxiv.org/abs/2309.00267)
- [Aligning Language Models to Follow Instructions](https://openai.com/research/instruction-following)

**Scaling & Evaluation:**
- [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)
- [AlpacaFarm: A Simulation Framework](https://arxiv.org/abs/2305.14387)
- [How Far Can Camels Go?](https://arxiv.org/abs/2306.04751)

**Learning Resources:**
- [Illustrating RLHF](https://huggingface.co/blog/rlhf)
- [LLM Training: RLHF and Alternatives](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives)
- [Preference Tuning LLMs](https://huggingface.co/blog/pref-tuning)
- [Fine-tune with DPO](https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html)
- [Fine-tune with GRPO](https://huggingface.co/learn/llm-course/en/chapter12/5)
- [DPO Wandb Logs](https://wandb.ai/alexander-vishnevskiy/dpo/reports/TRL-Original-DPO--Vmlldzo1NjI4MTc4)
- [Why you should work on AI AGENTS!](https://www.youtube.com/watch?v=fqVLjtvWgq8)
- [Deep Reinforcement Learning](https://www.youtube.com/playlist?list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc)
- [Reinforcement Learning Lecture Series (DeepMind)](https://www.youtube.com/playlist?list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm)
- [Reinforcement Learning (Polytechnique Montreal, Fall 2021)](https://www.youtube.com/playlist?list=PLImtCgowF_ES_JdF_UcM60EXTcGZg67Ua)
- [Foundations of Deep RL](https://www.youtube.com/playlist?list=PLwRJQ4m4UJjNymuBM9RdmB3Z9N5-0IlY0)
- [Stanford CS234: Reinforcement Learning](https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u)
- [Advanced Robotics: UC Berkeley](https://www.youtube.com/playlist?list=PLwRJQ4m4UJjNBPJdt8WamRAt4XKc639wF)
- [Stanford CS330: Deep Multi-Task and Meta Learning](https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5)

### Model Architectures

**Sparse & Efficient Architectures:**
- [Mixture of Experts](https://arxiv.org/abs/1701.06538)
- [Flash Attention Paper](https://arxiv.org/abs/2205.14135)

**State Space Models:**
- [Mamba Paper](https://arxiv.org/abs/2312.00752)
- [RWKV Paper](https://arxiv.org/abs/2305.13048)

**Long Context Models:**
- [Longformer Paper](https://arxiv.org/abs/2004.05150)
- [BigBird Paper](https://arxiv.org/abs/2007.14062)

**Positional Encodings:**
- [RoPE Paper](https://arxiv.org/abs/2104.09864)
- [ALiBi Paper](https://arxiv.org/abs/2108.12409)

### Reasoning

**Core Reasoning Papers:**
- [Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903)
- [Tree of Thoughts](https://arxiv.org/abs/2305.10601)
- [Self-Consistency](https://arxiv.org/abs/2203.11171)

**Tool Use & Action:**
- [ReAct Paper](https://arxiv.org/abs/2210.03629)
- [Program-Aided Language Models](https://arxiv.org/abs/2211.10435)
- [Tool Learning with LLMs](https://arxiv.org/abs/2304.08354)

**Evaluation Datasets:**
- [GSM8K Dataset](https://github.com/openai/grade-school-math)
- [MATH Dataset](https://github.com/hendrycks/math)
- [HumanEval](https://github.com/openai/human-eval)

**Advanced Reasoning Systems:**
- [Learning to Reason with LLMs](https://openai.com/index/learning-to-reason-with-llms/)
- [OpenAI o1 System Card](https://cdn.openai.com/o1-system-card-20241205.pdf)
- [DeepSeek-R1: Reinforcement Learning for Reasoning](https://arxiv.org/pdf/2501.12948)

**Resources:**
- [Intuitive Explanation of SAEs](https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html)
- [Scaling Test-time Compute](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute)

### Evaluation

**Standard Benchmarks:**
- [GLUE](https://gluebenchmark.com/)
- [SuperGLUE](https://super.gluebenchmark.com/)
- [BigBench](https://github.com/google/BIG-bench)
- [MMLU](https://github.com/hendrycks/test)
- [HellaSwag](https://rowanzellers.com/hellaswag/)

**Evaluation Frameworks:**
- [HELM](https://crfm.stanford.edu/helm/)
- [EleutherAI Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)
- [AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/)
- [Evaluation Guidebook](https://github.com/huggingface/evaluation-guidebook)
- [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)
- [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)
- [Lighteval](https://github.com/huggingface/lighteval)
- [Chatbot Arena](https://lmarena.ai/)
- [Ragas](https://github.com/explodinggradients/ragas/tree/main)
- [DeepEval](https://github.com/confident-ai/deepeval)

**Specialized Evaluation:**
- [TruthfulQA](https://github.com/sylinrl/TruthfulQA)
- [OpenBookQA](https://allenai.org/data/open-book-qa)
- [WebShop: Scalable Real-World Web Interaction](https://arxiv.org/pdf/2207.01206)
- [SWE-bench: GitHub Issues Resolution](https://arxiv.org/abs/2310.06770)
- [Tau-bench: Tool-Agent-User Interaction](https://arxiv.org/abs/2406.12045)

**LLM-as-Judge:**
- [Judging LLM-as-a-Judge with MT-Bench](https://proceedings.neurips.cc/paper_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf)

**Research & Methodology:**
- [Challenges and Opportunities in NLP Benchmarking](https://www.ruder.io/nlp-benchmarking/)
- [Measuring Massive Multitask Language Understanding](https://arxiv.org/abs/2009.03300)
- [Holistic Evaluation of Language Models](https://arxiv.org/abs/2211.09110)

## Part 3: LLM Engineer 🚀

### Quantization

**Quantization Libraries:**
- [llama.cpp](https://github.com/ggerganov/llama.cpp)
- [BitsAndBytes](https://github.com/TimDettmers/bitsandbytes)

**Advanced Quantization Methods:**
- [GPTQ](https://github.com/IST-DASLab/gptq)
- [AWQ](https://github.com/mit-han-lab/llm-awq)
- [SmoothQuant](https://arxiv.org/abs/2211.10438)
- [ZeroQuant](https://arxiv.org/abs/2206.01861)

**Formats & Standards:**
- [GGUF Format](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)
- [EXL2](https://github.com/turboderp/exllamav2)

**Learning Resources:**
- [4-bit LLM Quantization with GPTQ](https://mlabonne.github.io/blog/posts/4_bit_Quantization_with_GPTQ.html)
- [Quantize Llama models with llama.cpp](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html)
- [ExLlamaV2: Fastest Library to Run LLMs](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html)
- [Understanding AWQ](https://medium.com/friendliai/understanding-activation-aware-weight-quantization-awq-boosting-inference-serving-efficiency-in-10bb0faf63a8)
- [SmoothQuant on Llama 2](https://github.com/mit-han-lab/smoothquant/blob/main/examples/smoothquant_llama_demo.ipynb)
- [DeepSpeed Model Compression](https://www.deepspeed.ai/tutorials/model-compression/)
- [GPTQ Paper](https://arxiv.org/abs/2210.17323)
- [AWQ Paper](https://arxiv.org/abs/2306.00978)

### Inference Optimization

**High-Performance Inference Engines:**
- [vLLM](https://github.com/vllm-project/vllm)
- [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)
- [DeepSpeed-Inference](https://github.com/microsoft/DeepSpeed)
- [Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference)
- [MLC LLM](https://github.com/mlc-ai/mlc-llm)
- [mnn-llm](https://github.com/wangzhaode/mnn-llm/blob/master/README_en.md)

**Attention Optimization:**
- [Flash Attention](https://github.com/Dao-AILab/flash-attention)
- [PagedAttention](https://arxiv.org/abs/2309.06180)

**Advanced Techniques:**
- [Speculative Decoding](https://arxiv.org/abs/2211.17192)
- [Continuous Batching](https://www.anyscale.com/blog/continuous-batching-llm-inference)

**Learning Resources:**
- [Optimizing Latency](https://hamel.dev/notes/llm/inference/03_inference.html)
- [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one)
- [LLM Inference Best Practices](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices)
- [Optimizing LLMs for Speed and Memory](https://huggingface.co/docs/transformers/main/en/llm_tutorial_optimization)
- [Assisted Generation](https://huggingface.co/blog/assisted-generation)

### Model Enhancement

**Context Window Extension:**
- [YaRN](https://arxiv.org/abs/2309.00071)
- [Position Interpolation](https://arxiv.org/abs/2306.15595)

**Model Merging & Composition:**
- [Model Merging](https://arxiv.org/abs/2203.05482)
- [SLERP Merging](https://arxiv.org/abs/2306.01708)
- [TIES-Merging](https://arxiv.org/abs/2306.01708)
- [DARE](https://arxiv.org/abs/2311.03099)

**Knowledge Transfer:**
- [Knowledge Distillation](https://arxiv.org/abs/1503.02531)
- [Continual Learning](https://arxiv.org/abs/1909.08383)

**Learning Resources:**
- [Merge LLMs with MergeKit](https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit.html)
- [Create MoEs with MergeKit](https://mlabonne.github.io/blog/posts/2024-03-28_Create_Mixture_of_Experts_with_MergeKit.html)
- [Uncensor any LLM with Abliteration](https://mlabonne.github.io/blog/posts/2024-06-04_Uncensor_any_LLM_with_abliteration.html)
- [Smol Vision](https://github.com/merveenoyan/smol-vision)
- [Large Multimodal Models](https://huyenchip.com/2023/10/10/multimodal.html)

### Security & Responsible AI

**Security Frameworks:**
- [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/)

**Attack Vectors & Defense:**
- [Prompt Injection](https://arxiv.org/abs/2310.12815)
- [Jailbreaking LLMs](https://arxiv.org/abs/2307.15043)
- [Red Teaming LLMs](https://arxiv.org/abs/2209.07858)

**Safety & Evaluation:**
- [Alignment Tax](https://arxiv.org/abs/2211.15006)
- [AI Safety Benchmarks](https://arxiv.org/abs/2404.12241)

**Privacy Protection:**
- [Differential Privacy](https://arxiv.org/abs/1909.01917)
- [Federated Learning](https://arxiv.org/abs/1602.05629)

**Learning Resources:**
- [Prompt Injection Primer](https://github.com/jthack/PIPE)
- [LLM Security](https://llmsecurity.net/)
- [Red Teaming LLMs](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming)
- [garak](https://github.com/leondz/garak/)
- [langfuse](https://github.com/langfuse/langfuse)

**Interpretability Research:**
- [BERT Rediscovers the Classical NLP Pipeline](https://aclanthology.org/P19-1452/)
- [Axiomatic Attribution for Deep Networks](https://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf)
- [Faithful, Interpretable Model Explanations via Causal Abstraction](https://ai.stanford.edu/blog/causal-abstraction/)
- [Investigating Gender Bias Using Causal Mediation Analysis](https://proceedings.neurips.cc/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf)

### Running LLMs

**Web Frameworks:**
- [FastAPI](https://fastapi.tiangolo.com/)
- [Flask](https://flask.palletsprojects.com/)
- [Streamlit](https://streamlit.io/)
- [Gradio](https://gradio.app/)

**LLM APIs:**
- [OpenAI API](https://platform.openai.com/docs/api-reference)
- [Anthropic API](https://docs.anthropic.com/claude/reference/getting-started-with-the-api)
- [Hugging Face Inference API](https://huggingface.co/docs/api-inference/index)
- [Google Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview)
- [OpenRouter](https://openrouter.ai/)
- [Together AI](https://www.together.ai/)
- [Hugging Face Hub](https://huggingface.co/models)
- [Hugging Face Spaces](https://huggingface.co/spaces)

**Local LLM Tools:**
- [Ollama](https://ollama.ai/)
- [LM Studio](https://lmstudio.ai/)
- [Text Generation WebUI](https://github.com/oobabooga/text-generation-webui)
- [kobold.cpp](https://github.com/LostRuins/koboldcpp)

**Development Tools:**
- [LLM AutoEval](https://github.com/mlabonne/llm-autoeval)
- [LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing)
- [LazyAxolotl](https://colab.research.google.com/drive/1TsDKNo2riwVmU55gjuBgB1AXVtRRfRHW?usp=sharing)
- [AutoQuant](https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing)
- [Model Family Tree](https://colab.research.google.com/drive/1s2eQlolcI1VGgDhqWIANfkfKvcKrMyNr?usp=sharing)
- [ZeroSpace](https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC)
- [AutoAbliteration](https://colab.research.google.com/drive/1RmLv-pCMBBsQGXQIM8yF-OdCNyoylUR1?usp=sharing)
- [AutoDedup](https://colab.research.google.com/drive/1o1nzwXWAa8kdkEJljbJFW1VuI-3VZLUn?usp=sharing)

**Technologies:**
- [Supabase](https://supabase.com/) - Database, authentication, storage, and realtime
- [LangChain](https://langchain.com/) - Building RAG pipelines
- [PostHog](https://posthog.com/) - Analytics
- [FastAPI](https://fastapi.tiangolo.com/) - Backend framework
- [Next.js](https://nextjs.org/) - Frontend framework
- [Resend](https://resend.com/) - Email service
- [LiteLLM](https://litellm.ai/) - LLM compatibility layer
- [Ollama](https://ollama.ai/) - Local LLM serving
- [Mistral AI](https://mistral.ai/) - Open source LLMs

**Educational Platforms:**
- [Gradescope](https://www.gradescope.com/)
- [Stanford Canvas](https://canvas.stanford.edu/)
- [Ed Forum](https://edstem.org/)
- [Stanford CS224N Project Reports](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1244/project.html)

**Learning Resources:**
- [Run LLM with LM Studio](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio)
- [Prompt Engineering Guide](https://www.promptingguide.ai/)
- [Outlines - Quickstart](https://dottxt-ai.github.io/outlines/latest/quickstart/)
- [LMQL Overview](https://lmql.ai/docs/language/overview.html)
- [Streamlit - Build LLM App](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps)
- [HF LLM Inference Container](https://huggingface.co/blog/sagemaker-huggingface-llm)
- [Philschmid Tutorials](https://www.philschmid.de/)
- [SkyPilot](https://skypilot.readthedocs.io/en/latest/)
- [LLMOps: Building Real-World Applications With Large Language Models](https://www.comet.com/site/llm-course/)
- [Evaluating and Debugging Generative AI](https://www.deeplearning.ai/short-courses/evaluating-debugging-generative-ai/)
- [LangChain for LLM Application Development](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/)
- [LangChain: Chat with Your Data](https://www.deeplearning.ai/short-courses/building-systems-with-chatgpt/)
- [Building Systems with the ChatGPT API](https://www.deeplearning.ai/short-courses/building-systems-with-chatgpt/)
- [LangChain & Vector Databases in Production](https://learn.activeloop.ai/courses/langchain)
- [Building LLM-Powered Apps](https://www.wandb.courses/courses/building-llm-powered-apps)
- [Full Stack LLM Bootcamp](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/)
- [Full Stack Deep Learning](https://www.youtube.com/playlist?list=PL1T8fO7ArWlcWg04OgNiJy91PywMKT2lv)
- [Practical Deep Learning for Coders - Part 1](https://www.youtube.com/playlist?list=PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU)
- [Practical Deep Learning for Coders - Part 2](https://www.youtube.com/watch?v=_7rMfsA24Ls&ab_channel=JeremyHoward)
- [Stanford MLSys Seminars](https://www.youtube.com/playlist?list=PLSrTvUm384I9PV10koj_cqit9OfbJXEkq)
- [Machine Learning Engineering for Production (MLOps)](https://www.youtube.com/playlist?list=PLkDaE6sCZn6GMoA0wbpJLi3t34Gd8l0aK)
- [MIT Introduction to Data-Centric AI](https://www.youtube.com/watch?v=ayzOzZGHZy4&list=PLnSYPjg2dHQKdig0vVbN-ZnEU0yNJ1mo5)
- [llm-course](https://github.com/mlabonne/llm-course)
- [GPT in 60 Lines of NumPy](https://jaykmody.com/blog/gpt-from-scratch/)
- [femtoGPT](https://github.com/keyvank/femtoGPT)
- [Master and Build Large Language Models](https://www.manning.com/livevideo/master-and-build-large-language-models)
- [Test Yourself On Build a Large Language Model (From Scratch)](https://www.manning.com/books/test-yourself-on-build-a-large-language-model-from-scratch)
- [PyTorch in One Hour: From Tensors to Training Neural Networks on Multiple GPUs](https://sebastianraschka.com/teaching/pytorch-1h/)

### RAG

**RAG Frameworks:**
- [LangChain](https://python.langchain.com/docs/get_started/introduction)
- [LlamaIndex](https://docs.llamaindex.ai/en/stable/)
- [Haystack](https://haystack.deepset.ai/)

**Vector Databases:**
- [Pinecone](https://www.pinecone.io/)
- [Weaviate](https://weaviate.io/)
- [Chroma](https://www.trychroma.com/)
- [Qdrant](https://qdrant.tech/)
- [FAISS](https://github.com/facebookresearch/faiss)
- [Milvus](https://milvus.io/)

**Graph RAG:**
- [Neo4j](https://neo4j.com/)
- [Graph RAG](https://arxiv.org/abs/2404.16130)

**Foundational RAG Papers:**
- [Dense Passage Retrieval (DPR)](https://arxiv.org/abs/2004.04906)
- [REALM](https://arxiv.org/abs/2002.08909)
- [Reading Wikipedia to Answer Open-Domain Questions](https://arxiv.org/abs/1704.00051)

**Advanced RAG Research:**
- [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)
- [In-Context Retrieval-Augmented Language Models](https://arxiv.org/abs/2302.00083)
- [Scaling Retrieval-Based Language Models](https://arxiv.org/abs/2407.12854)
- [SILO Language Models](https://arxiv.org/abs/2308.04430)

**Question Answering:**
- [SQuAD: 100,000+ Questions for Machine Comprehension](https://arxiv.org/abs/1606.05250)
- [Bidirectional Attention Flow for Machine Comprehension](https://arxiv.org/abs/1611.01603)
- [Adversarial Examples for Evaluating Reading Comprehension](https://aclanthology.org/D17-1215/)

**Learning Resources:**
- [LangChain Text Splitters](https://python.langchain.com/docs/how_to/#text-splitters)
- [Top 7 Vector Databases](https://www.datacamp.com/blog/the-top-5-vector-databases)
- [LlamaIndex High-level Concepts](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html)
- [Model Context Protocol](https://modelcontextprotocol.io/introduction)
- [Pinecone Retrieval Augmentation](https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/)
- [LangChain Q&A with RAG](https://python.langchain.com/docs/tutorials/rag/)
- [LangChain Memory Types](https://python.langchain.com/docs/how_to/chatbots_memory/)
- [RAG Pipeline Metrics](https://docs.ragas.io/en/stable/concepts/metrics/index.html)
- [LangChain Query Construction](https://blog.langchain.dev/query-construction/)
- [LangChain SQL Tutorials](https://python.langchain.com/docs/tutorials/sql_qa/)
- [Applying OpenAI's RAG](https://blog.langchain.dev/applying-openai-rag/)
- [DSPy in 8 Steps](https://dspy-docs.vercel.app/docs/building-blocks/solving_your_task)
- [Improve ChatGPT with Knowledge Graphs](https://mlabonne.github.io/blog/posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html)
- [RAG-Fusion](https://github.com/Raudaschl/rag-fusion)
- [DSPy](https://github.com/stanfordnlp/dspy)

### Agents

**Agent Frameworks:**
- [LangGraph](https://langchain-ai.github.io/langgraph/concepts/why-langgraph/)
- [AutoGen](https://github.com/microsoft/autogen)
- [CrewAI](https://github.com/joaomdmoura/crewai)
- [LangChain Agents](https://python.langchain.com/docs/modules/agents/)
- [LlamaIndex Agents](https://docs.llamaindex.ai/en/stable/use_cases/agents/)
- [smolagents](https://huggingface.co/docs/smolagents/index)

**Function Calling & Tools:**
- [OpenAI Function Calling](https://platform.openai.com/docs/guides/function-calling)
- [ReAct Implementation](https://github.com/ysymyth/ReAct)
- [OpenAI Agents SDK](https://github.com/openai/openai-agents-python)

**Microsoft Frameworks:**
- [Semantic Kernel](https://github.com/microsoft/semantic-kernel)
- [TaskWeaver](https://github.com/microsoft/TaskWeaver)

**Learning Resources:**
- [Agents Course](https://huggingface.co/learn/agents-course/unit0/introduction)
- [AI Agents Comparison](https://langfuse.com/blog/2025-03-19-ai-agent-comparison)
- [Pinecone LLM Agents](https://www.pinecone.io/learn/series/langchain/langchain-agents/)
- [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/)

### Multimodal

**Vision-Language Models:**
- [CLIP](https://github.com/openai/CLIP)
- [LLaVA](https://github.com/haotian-liu/LLaVA)
- [GPT-4V](https://openai.com/research/gpt-4v-system-card)
- [Flamingo](https://arxiv.org/abs/2204.14198)
- [BLIP](https://arxiv.org/abs/2201.12086)

**Audio Processing:**
- [Whisper](https://github.com/openai/whisper)
- [torchaudio](https://pytorch.org/audio/)

**Image Generation:**
- [Stable Diffusion](https://github.com/CompVis/stable-diffusion)
- [DALL-E](https://openai.com/dall-e-2)

**Processing Libraries:**
- [OpenCV](https://opencv.org/)
- [Pillow](https://pillow.readthedocs.io/)

**Learning Resources:**
- [Large Multimodal Models](https://huyenchip.com/2023/10/multimodal.html)
- [Smol Vision](https://github.com/merveenoyan/smol-vision)
- [CS231N: Convolutional Neural Networks for Visual Recognition](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)
- [Deep Learning for Computer Vision](https://www.youtube.com/playlist?list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r)
- [Deep Learning for Computer Vision (DL4CV)](https://www.youtube.com/playlist?list=PL_Z2_U9MIJdNgFM7-f2fZ9ZxjVRP_jhJv)
- [Deep Learning for Computer Vision (neuralearn.ai)](https://www.youtube.com/watch?v=IA3WxTTPXqQ)
- [AMMI Geometric Deep Learning Course](https://www.youtube.com/playlist?list=PLn2-dEmQeTfSLXW8yXP4q_Ii58wFdxb3C)

### LLMOps

**MLOps Platforms:**
- [MLflow](https://mlflow.org/)
- [Weights & Biases](https://wandb.ai/)
- [Kubeflow](https://www.kubeflow.org/)

**Infrastructure & Orchestration:**
- [Docker](https://www.docker.com/)
- [Kubernetes](https://kubernetes.io/)
- [Terraform](https://www.terraform.io/)
- [Apache Airflow](https://airflow.apache.org/)

**Monitoring & Observability:**
- [Prometheus](https://prometheus.io/)
- [Grafana](https://grafana.com/)
- [langfuse](https://github.com/langfuse/langfuse)

**Data Processing:**
- [Apache Spark](https://spark.apache.org/)
- [Databricks](https://databricks.com/)

**CI/CD & Model Management:**
- [GitHub Actions](https://github.com/features/actions)
- [Hugging Face Hub](https://huggingface.co/docs/hub/index)
