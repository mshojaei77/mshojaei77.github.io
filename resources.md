# Learning Resources

## 1. Tokenization

- [Introduction to Tokenization](https://medium.com/@mshojaei77/introduction-to-tokenization-a-theoretical-perspective-b1cc22fe98c5)
- [Understanding BPE Tokenization](https://medium.com/@mshojaei77/understanding-bpe-tokenization-a-hands-on-tutorial-80570314b12f)
- [Fast Tokenizers: How Rust is Turbocharging NLP](https://medium.com/@mshojaei77/fast-tokenizers-how-rust-is-turbocharging-nlp-dd12a1d13fa9)
- [Tokenization Techniques](https://colab.research.google.com/drive/1RwrtINbHTPBSRIoW8Zn9BRabxXguRRf0?usp=sharing)
- [GPT Tokenizer Implementation](https://colab.research.google.com/drive/1y0KnCFZvGVf_odSfcNAws6kcDD7HsI0L?usp=sharing)
- [Build and Push a Tokenizer](https://colab.research.google.com/drive/1uYFoxwCKwshkchBgQ4y4z9cDfKRlwZ-e?usp=sharing)
- [Tokenizer Comparison](https://colab.research.google.com/drive/1wVSCBGFm7KjJy-KugYGYETpncWsPgx5N?usp=sharing)
- [Hugging Face Tokenizers](https://colab.research.google.com/drive/1mcFgQ9PX1TFyEAsFOnoS1ozeSz3vM6A1?usp=sharing)
- [New Tokenizer Training](https://colab.research.google.com/drive/1452WFn66MZzYylTNcL6hV5Zd45sskzs7?usp=sharing)
- [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)
- [TikTokenizer](https://tiktokenizer.vercel.app/)
- [Hugging Face Tokenizer](https://huggingface.co/spaces/Xenova/the-tokenizer-playground)
- [OpenAI Tokenizer](https://platform.openai.com/tokenizer)
- [Tokenizer Arena](https://huggingface.co/spaces/Cognitive-Lab/Tokenizer_Arena)
- [Hugging Face Tokenizers](https://huggingface.co/docs/tokenizers/mastering-tokenizers)
- [SentencePiece](https://github.com/google/sentencepiece)
- [BPE Research Paper](https://arxiv.org/abs/1508.07909)
- [RadarLLM: Cross-Modal Tokenization](https://arxiv.org/abs/2504.09862)
- [CoreMatching: Token-Neuron Synergy](https://arxiv.org/abs/2505.19235)
- [MOM: Memory-Efficient Token Handling](https://arxiv.org/abs/2504.12526)

## 2. Embeddings

- [Word Embeddings Deep Dive](https://medium.com/@mshojaei77/from-words-to-vectors-a-gentle-introduction-to-word-embeddings-eaadb1654778)
- [Contextual Embedding Guide](https://medium.com/@mshojaei77/beyond-one-word-one-meaning-contextual-embeddings-187b48c6fc27)
- [Sentence Embedding Techniques](https://medium.com/@mshojaei77/beyond-words-mastering-sentence-embeddings-for-semantic-nlp-dc852b1382ba)
- [Interactive Word2Vec Tutorial](https://colab.research.google.com/drive/1dVkCRF0RKWWSP_QQq79LHNYGhead14d0?usp=sharing)
- [Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/)
- [Word2vec from Scratch](https://jaketae.github.io/study/word2vec/)
- [Training Sentence Transformers](https://huggingface.co/blog/train-sentence-transformers)
- [CS224N Lecture 1 - Word Vectors](https://www.youtube.com/watch?v=rmVRLeJRkl4)
- [BERT Paper](https://arxiv.org/abs/2204.03503)
- [GloVe Paper](https://www.semanticscholar.org/paper/67b692bbfd29c5a30cfd1046efd5f85eecd1ea86)
- [FastText Paper](https://www.semanticscholar.org/paper/d23e59abcae6ba653ba45dcc0ef975438890a3a4)
- [Multilingual BERT](https://www.semanticscholar.org/paper/0b0bc70b48aebe608d53a955990cb08f73de5a7d)
- [Bias in Embeddings](https://www.semanticscholar.org/paper/5ea2104a039921633f75a9f4b986b515ddbe96d7)

## 3. Neural Network Foundations for LLMs

- [Deep Learning Book](https://www.deeplearningbook.org/)
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
- [Stanford CS231n](http://cs231n.stanford.edu/)
- [TensorFlow Guide](https://www.tensorflow.org/guide)
- [PyTorch Tutorials](https://pytorch.org/tutorials/)

## 4. Traditional Language Models

- [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)
- [Statistical Language Models](https://www.cambridge.org/core/books/statistical-language-models)
- [Neural Probabilistic Language Models](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
- [Deep Learning Book - RNNs](https://www.deeplearningbook.org/contents/rnn.html)
- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [Empirical Evaluation of Gated RNNs](https://arxiv.org/abs/1412.3555)
- [Stanford CS224n](http://web.stanford.edu/class/cs224n/)
- [Oxford Deep NLP](https://github.com/oxford-cs-deepnlp-2017/lectures)

## 5. The Transformer Architecture

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [Transformer Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)
- [Understanding Layer Normalization](https://arxiv.org/abs/1607.06450)
- [PyTorch Transformer Tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)
- [TensorFlow Transformer Tutorial](https://www.tensorflow.org/tutorials/text/transformer)

## 6. Data Preparation

- [Common Crawl Documentation](https://commoncrawl.org/the-data/)
- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Scrapy Documentation](https://scrapy.org/)
- [Hugging Face Datasets Guide](https://huggingface.co/docs/datasets/)
- [Stanford Text Preprocessing](https://nlp.stanford.edu/IR-book/html/htmledition/text-preprocessing-1.html)
- [Apache Beam](https://beam.apache.org/)
- [Data Quality for Machine Learning](https://www.amazon.com/Data-Quality-Machine-Learning-Practices/dp/1492094964)
- [Google's Data Preparation](https://cloud.google.com/architecture/data-preprocessing-for-ml-with-tf-transform-pt1)
- [Data Ethics Framework](https://www.gov.uk/government/publications/data-ethics-framework)
- [Responsible AI Practices](https://ai.google/responsibilities/responsible-ai-practices/)
- [DVC (Data Version Control)](https://dvc.org/)
- [Pandas Documentation](https://pandas.pydata.org/)
- [Elasticsearch](https://www.elastic.co/elasticsearch/)

## 7. Pre-Training Large Language Models

- [Understanding Language Models](https://medium.com/@mshojaei77/1ac0e05ca1f3)
- [LLMs Overview](https://arxiv.org/pdf/2307.06435)
- [LLMs Survey](https://arxiv.org/abs/2402.06196)
- [LLMs Explained Briefly](https://www.youtube.com/watch?v=LPZh9BOjkQs)
- [Deep Dive into LLMs](https://www.youtube.com/watch?v=7xTGNNLPyMI)
- [Intro to LLMs](https://www.youtube.com/watch?v=zjkBMFhNj_g)
- [DeepSpeed](https://www.deepspeed.ai/)
- [Ray](https://ray.io/)
- [Triton](https://triton-lang.org/)
- [ONNX](https://onnx.ai/)

## 8. Post-Training Datasets

- [Alpaca Dataset](https://github.com/tatsu-lab/stanford_alpaca)
- [ShareGPT Dataset](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered)
- [Dolly Dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k)
- [OpenAssistant Conversations](https://huggingface.co/datasets/OpenAssistant/oasst1)
- [Anthropic HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf)

## 9. Supervised Fine-Tuning (SFT)

- [PEFT Library](https://github.com/huggingface/peft)
- [Unsloth](https://github.com/unslothai/unsloth)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [QLoRA Paper](https://arxiv.org/abs/2305.14314)
- [Alpaca Fine-tuning](https://github.com/tatsu-lab/stanford_alpaca)
- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)

## 10. Preference Alignment (RL Fine-Tuning)

- [TRL (Transformer Reinforcement Learning)](https://github.com/huggingface/trl)
- [Ray RLlib](https://docs.ray.io/en/latest/rllib/)
- [InstructGPT Paper](https://arxiv.org/abs/2203.02155)
- [PPO Paper](https://arxiv.org/abs/1707.06347)
- [DPO Paper](https://arxiv.org/abs/2305.18290)
- [Constitutional AI Paper](https://arxiv.org/abs/2212.08073)
- [RLAIF Paper](https://arxiv.org/abs/2309.00267)

## 11. Model Architecture Variants

- [Mixture of Experts](https://arxiv.org/abs/1701.06538)
- [Mamba Paper](https://arxiv.org/abs/2312.00752)
- [RWKV Paper](https://arxiv.org/abs/2305.13048)
- [Longformer Paper](https://arxiv.org/abs/2004.05150)
- [BigBird Paper](https://arxiv.org/abs/2007.14062)
- [Flash Attention Paper](https://arxiv.org/abs/2205.14135)
- [RoPE Paper](https://arxiv.org/abs/2104.09864)
- [ALiBi Paper](https://arxiv.org/abs/2108.12409)

## 12. Reasoning

- [Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903)
- [Tree of Thoughts](https://arxiv.org/abs/2305.10601)
- [ReAct Paper](https://arxiv.org/abs/2210.03629)
- [Self-Consistency](https://arxiv.org/abs/2203.11171)
- [Program-Aided Language Models](https://arxiv.org/abs/2211.10435)
- [Tool Learning with LLMs](https://arxiv.org/abs/2304.08354)
- [GSM8K Dataset](https://github.com/openai/grade-school-math)
- [MATH Dataset](https://github.com/hendrycks/math)
- [HumanEval](https://github.com/openai/human-eval)

## 13. Model Evaluation

- [GLUE](https://gluebenchmark.com/)
- [SuperGLUE](https://super.gluebenchmark.com/)
- [BigBench](https://github.com/google/BIG-bench)
- [HELM](https://crfm.stanford.edu/helm/)
- [MMLU](https://github.com/hendrycks/test)
- [HellaSwag](https://rowanzellers.com/hellaswag/)
- [TruthfulQA](https://github.com/sylinrl/TruthfulQA)
- [OpenBookQA](https://allenai.org/data/open-book-qa)
- [EleutherAI Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)

## 14. Quantization

- [llama.cpp](https://github.com/ggerganov/llama.cpp)
- [GPTQ](https://github.com/IST-DASLab/gptq)
- [AWQ](https://github.com/mit-han-lab/llm-awq)
- [BitsAndBytes](https://github.com/TimDettmers/bitsandbytes)
- [SmoothQuant](https://arxiv.org/abs/2211.10438)
- [ZeroQuant](https://arxiv.org/abs/2206.01861)
- [GGUF Format](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)

## 15. Inference Optimization

- [vLLM](https://github.com/vllm-project/vllm)
- [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)
- [DeepSpeed-Inference](https://github.com/microsoft/DeepSpeed)
- [Flash Attention](https://github.com/Dao-AILab/flash-attention)
- [PagedAttention](https://arxiv.org/abs/2309.06180)
- [Speculative Decoding](https://arxiv.org/abs/2211.17192)
- [Continuous Batching](https://www.anyscale.com/blog/continuous-batching-llm-inference)

## 16. Model Enhancement

- [YaRN](https://arxiv.org/abs/2309.00071)
- [Position Interpolation](https://arxiv.org/abs/2306.15595)
- [Model Merging](https://arxiv.org/abs/2203.05482)
- [SLERP Merging](https://arxiv.org/abs/2306.01708)
- [TIES-Merging](https://arxiv.org/abs/2306.01708)
- [DARE](https://arxiv.org/abs/2311.03099)
- [Knowledge Distillation](https://arxiv.org/abs/1503.02531)
- [Continual Learning](https://arxiv.org/abs/1909.08383)

## 17. Securing LLMs & Responsible AI

- [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [Prompt Injection](https://arxiv.org/abs/2310.12815)
- [Jailbreaking LLMs](https://arxiv.org/abs/2307.15043)
- [Red Teaming LLMs](https://arxiv.org/abs/2209.07858)
- [Alignment Tax](https://arxiv.org/abs/2211.15006)
- [AI Safety Benchmarks](https://arxiv.org/abs/2404.12241)
- [Differential Privacy](https://arxiv.org/abs/1909.01917)
- [Federated Learning](https://arxiv.org/abs/1602.05629)

## 18. Running LLMs & Building Applications

- [FastAPI](https://fastapi.tiangolo.com/)
- [Streamlit](https://streamlit.io/)
- [Gradio](https://gradio.app/)
- [Flask](https://flask.palletsprojects.com/)
- [OpenAI API](https://platform.openai.com/docs/api-reference)
- [Anthropic API](https://docs.anthropic.com/claude/reference/getting-started-with-the-api)
- [Hugging Face Inference API](https://huggingface.co/docs/api-inference/index)
- [Ollama](https://ollama.ai/)
- [LM Studio](https://lmstudio.ai/)
- [Text Generation WebUI](https://github.com/oobabooga/text-generation-webui)

## 19. Retrieval Augmented Generation (RAG)

- [LangChain](https://python.langchain.com/docs/get_started/introduction)
- [LlamaIndex](https://docs.llamaindex.ai/en/stable/)
- [Haystack](https://haystack.deepset.ai/)
- [Pinecone](https://www.pinecone.io/)
- [Weaviate](https://weaviate.io/)
- [Chroma](https://www.trychroma.com/)
- [Qdrant](https://qdrant.tech/)
- [FAISS](https://github.com/facebookresearch/faiss)
- [Milvus](https://milvus.io/)
- [Neo4j](https://neo4j.com/)
- [Graph RAG](https://arxiv.org/abs/2404.16130)

## 20. Tool Use & AI Agents

- [LangGraph](https://langchain-ai.github.io/langgraph/)
- [AutoGen](https://github.com/microsoft/autogen)
- [CrewAI](https://github.com/joaomdmoura/crewai)
- [OpenAI Function Calling](https://platform.openai.com/docs/guides/function-calling)
- [ReAct Implementation](https://github.com/ysymyth/ReAct)
- [LangChain Agents](https://python.langchain.com/docs/modules/agents/)
- [Semantic Kernel](https://github.com/microsoft/semantic-kernel)
- [TaskWeaver](https://github.com/microsoft/TaskWeaver)

## 21. Multimodal LLMs

- [CLIP](https://github.com/openai/CLIP)
- [LLaVA](https://github.com/haotian-liu/LLaVA)
- [GPT-4V](https://openai.com/research/gpt-4v-system-card)
- [Whisper](https://github.com/openai/whisper)
- [Stable Diffusion](https://github.com/CompVis/stable-diffusion)
- [DALL-E](https://openai.com/dall-e-2)
- [Flamingo](https://arxiv.org/abs/2204.14198)
- [BLIP](https://arxiv.org/abs/2201.12086)
- [OpenCV](https://opencv.org/)
- [Pillow](https://pillow.readthedocs.io/)
- [torchaudio](https://pytorch.org/audio/)

## 22. Large Language Model Operations (LLMOps)

- [MLflow](https://mlflow.org/)
- [Weights & Biases](https://wandb.ai/)
- [Kubeflow](https://www.kubeflow.org/)
- [Docker](https://www.docker.com/)
- [Kubernetes](https://kubernetes.io/)
- [Terraform](https://www.terraform.io/)
- [Prometheus](https://prometheus.io/)
- [Grafana](https://grafana.com/)
- [Apache Spark](https://spark.apache.org/)
- [Databricks](https://databricks.com/)
- [Apache Airflow](https://airflow.apache.org/)
- [GitHub Actions](https://github.com/features/actions)
- [Hugging Face Hub](https://huggingface.co/docs/hub/index)
