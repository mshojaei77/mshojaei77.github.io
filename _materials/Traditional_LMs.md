---
title: "Traditional Language Models Tutorial: Pre-Transformers Era"
nav_order: 4
parent: Tutorials
layout: default
---

# Traditional Language Models Tutorial ðŸ“œ: Old School Word Predictors Before Transformers
Explore traditional LMs guide â€“ from n-grams to RNNs. Foundation for modern LLMs.

## N-Grams Guide
- Count previous words to predict next â€“ simple yet effective.
- Markov assumption: Limit history for efficiency.
- Smoothing techniques: Laplace for handling zeros.

## Feedforward Neural Language Models
- Fixed window neural nets.
- Embeddings + MLP for probability predictions.

## RNNs in Language Modeling
- LSTM/GRU: Tackle long dependencies like a boss.
- Seq2seq foundations for advanced tasks.

## Why Study Traditional LMs?
Understand the evolution to transformers! What's your take on RNNs vs. Transformers? Comment! ðŸ’¬

## My Traditional LMs Notes
- [Medium: N-Gram Basics](https://medium.com/@mshojaei77/understanding-n-gram-language-models-a-building-block-of-modern-nlp-3e4f3f0e1d3d)
- [Colab: N-Gram Playground](https://colab.research.google.com/drive/1y0KnCFZvGVf_odSfcNAws6kcDD7HsI0L?usp=sharing)

## Top Traditional LMs Resources
- [Jurafsky SLP Book](https://web.stanford.edu/~jurafsky/slp3/)
- [Karpathy on RNN Effectiveness](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- [LSTM Original Paper](https://www.bioinf.jku.at/publications/older/2604.pdf)
- [GRU Paper](https://arxiv.org/abs/1412.3555)
- [Colah's LSTM Post](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [Yoav Goldberg Primer](https://arxiv.org/abs/1807.10854)

Keywords: traditional language models tutorial, n-grams guide, RNN language models, pre-transformers LMs, AI word prediction