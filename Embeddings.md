---
title: "Embeddings"
nav_order: 4
---

# Embeddings

![image](https://github.com/user-attachments/assets/944f2cce-c66d-4c51-a443-cebc151055ff)

## Overview
Word embeddings are fundamental to Natural Language Processing, serving as dense vector representations of words that capture semantic and syntactic relationships. This module explores both static and contextual word embeddings, their evolution, and their applications in modern NLP.

## 1. Word Embeddings
Word embeddings are dense vector representations of words that capture semantic and syntactic relationships. Understanding these embeddings is crucial for various NLP tasks.

### Learning Materials
- **[ðŸ“„ Medium Article: Word Embeddings Deep Dive](https://lilianweng.github.io/posts/2017-10-15-word-embedding/)**
  - *Comprehensive overview of various word embedding techniques.*
- **[ðŸŸ  Colab Notebook: Word2Vec Implementation](https://colab.research.google.com/drive/yournotebooklink3)**
  - *Implement Word2Vec from scratch to understand its mechanics.*
- **[ðŸŸ  Colab Notebook: GloVe Implementation](https://colab.research.google.com/drive/yournotebooklink4)**
  - *Implement GloVe to learn about global co-occurrence statistics.*

## 2. Contextual Embeddings
Contextual embeddings vary based on the context in which words appear, capturing nuanced meanings. This section highlights the significance of these embeddings in modern NLP applications.

### Learning Materials
- **[ðŸ“„ Paper: BERT Paper](https://arxiv.org/abs/2204.03503)**
  - *Revolutionizing context: Understanding the architecture that advanced contextual word embeddings.*
- **[ðŸŸ  Colab Notebook: BERT Embeddings Exploration](https://colab.research.google.com/drive/yournotebooklink_bert_exploration)**
  - *Experiment with pre-trained BERT models to analyze contextual embeddings.*

## Additional Resources
[![Word Embeddings Deep Dive](https://badgen.net/badge/Blog/Word%20Embeddings%20Deep%20Dive/pink)](https://lilianweng.github.io/posts/2017-10-15-word-embedding/)
[![CS224N Lecture 1 - Intro & Word Vectors](https://badgen.net/badge/Video/CS224N%20Lecture%201%20-%20Intro%20&%20Word%20Vectors/red)](https://www.youtube.com/watch?v=rmVRLeJRkl4)
[![Illustrated Word2Vec](https://badgen.net/badge/Blog/Illustrated%20Word2Vec/pink)](https://jalammar.github.io/illustrated-word2vec/)
[![Contextual Embeddings](https://badgen.net/badge/Paper/Contextual%20Embeddings/purple)](https://www.cs.princeton.edu/courses/archive/spring20/cos598C/lectures/lec3-contextualized-word-embeddings.pdf)
[![Training Sentence Transformers](https://badgen.net/badge/Blog/Training%20Sentence%20Transformers/pink)](https://huggingface.co/blog/train-sentence-transformers)
[![BERT Paper](https://badgen.net/badge/Paper/BERT%20Paper/purple)](https://arxiv.org/abs/2204.03503)
[![GloVe Paper](https://badgen.net/badge/Paper/GloVe%20Paper/purple)](https://www.semanticscholar.org/paper/67b692bbfd29c5a30cfd1046efd5f85eecd1ea86)
[![FastText Paper](https://badgen.net/badge/Paper/FastText%20Paper/purple)](https://www.semanticscholar.org/paper/d23e59abcae6ba653ba45dcc0ef975438890a3a4)
[![Multilingual BERT Paper](https://badgen.net/badge/Paper/Multilingual%20BERT%20Paper/purple)](https://www.semanticscholar.org/paper/0b0bc70b48aebe608d53a955990cb08f73de5a7d)
[![Bias in Contextualized Word Embeddings](https://badgen.net/badge/Paper/Bias%20in%20Contextualized%20Embeddings/purple)](https://www.semanticscholar.org/paper/5ea2104a039921633f75a9f4b986b515ddbe96d7)